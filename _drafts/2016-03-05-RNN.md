---
layout: "post"
title: "Recurrent Neural Network"
excerpt: "RNN, LSTM"
date: "2016-03-05 10:00:00"
---

Recurrent Network가 특별한 이유가 무엇일까? 우리가(내가) 공부했던 Neural Network나 혹은 CNN은 너무 제한적이다. 그러니까 예를들어 CNN은 입력값으로 고정된 크기의 벡터만 받을 수 있으며(이미지) 고정된 계산 스텝(레이어의 개수)을 통해서 고정된 크기의 벡터를 출력한다. 반면 RNN은 벡터의 시퀀스를 처리할 수 있다는 점이다.

<div class="imgcap">
<img src="/assets/RNN/varient.jpg">
</div>

one-to-one은 일반적인 neural net이다. 고정된 크기의 벡터를 입력받고 출력하며, 이미지 분류 문제가 여기에 속한다. one-to-many는 입력은 고정된 크기의 벡터를 받지만 벡터의 시퀀스를 출력하며, 이미지를 입력받고 이를 captioning하는 문제를 이 모델을 통해 풀 수 있다. many-to-one은 텍스트를 입력받고 감정 분석 하는 문제를 예를 들수 있겠고, 왼쪽의 many-to-many는 기계 번역을 처리할 수 있다. 오른쪽의 many-to-many 모델은 입력과 출력간의 싱크가 맞춰져있는데 비디오를 보고 분류하는 문제를 이 모델을 활용하여 풀 수 있다.

입력 혹은 출력에 시퀀스가 주어지는것은 드문게 아닐까 하는 의문이 들 수 있다. 하지만 RNN은 입력/출력의 벡터가 고정된 크기에도 잘 동작한다. 아래의 그림들은 DeepMind에서 발표한 논문들인데, 왼쪽은 recurrent network를 이용해서 house number를 읽는 일을 한다. (아직 논문을 읽지 않아 정확히는 모르겠지만) attention을 이용해서 사진의 왼쪽부터 오른쪽으로 훑으면서 읽는 모습을 볼 수 있다. 반면 오른쪽은 recurrent network가 house number를 "그리는" 것이다.

<div class="imgcap">
<img src="/assets/RNN/house1.gif" style="max-width:49%">
<img src="/assets/RNN/house2.gif" style="max-width:49%">
</div>

이런 예시들은 recurrent network가 고정된 크기의 입/출력 벡터를 가지더라도 효과적으로 동작 할 수 있음을 보여준다.

## RNN computation
RNN은 다른 네트워크들과 비슷하게 입력으로 벡터 `x`를 받고, `y`를 출력한다. 하지만 다른점은 출력값은 그 time step에서의 입력과 예전 **모든 과거들의 값들에 영향**을 받는다는 것이다.

```python
rnn = RNN()
y = rnn.step(x) # x는 입력 벡터, y는 RNN의 출력 벡터이다.
```

그래서 RNN은 매 time step마다 업데이트되는 hidden state를 가지고 있다. 여기서는 간단하게 hidden state `h`를 한개만 가지고 있다고 가정해보자.

```python
class RNN:
  # ...
  def step(self, x):
    # hidden state를 업데이트
    self.h = np.tanh(np.dot(self.W_hh, self.h) + np.dot(self.W_xh, x))
    # 출력 벡터 계산
    y = np.dot(self.W_hy, self.h)

    return y
```

위 코드는 RNN의 foward pass를 간략하게 작성한 것이다. parameter는 `W_hh, W_xh, W_hy` 3개의 행렬이 있고 hidden state `self.h`는 zero 벡터로 초기화 하였으며, `np.tanh`는 tanh non-linearlity를 의미한다.


## 레퍼런스
[Andrej Karpathy의 블로그](http://karpathy.github.io/2015/05/21/rnn-effectiveness/)
