---
layout: "post"
title: "RL"
excerpt: "RL"
date: "2016-08-01 05:00:00"
---

## Table of contents

- [Introduction](#Intro)
- [Markov Decision Process](#MDP)
- [Planning by Dynamic Programming](#Planning)
- [Model-Free Prediction and Control](#Model-Free)

<h1 id="Intro">Introduction</h1>
---
머신 러닝은 크게 지도 학습과 비지도 학습 두 가지로 분류를 하곤 하는데 강화 학습은 두 갈래 모두에도 속하지 않는 머신 러닝 방법이다. 강화 학습이 다른 머신 러닝 방법과 다른 점은,

- 지도 (Supervisor)가 있는 지도 학습과 달리 **보상** (Reward)만 존재 한다.
- 결정 혹은 행동에 따른 피드백이 즉각적으로 오지 않는다.
- 대부분 시간과 관련되어 있으며, 시간에 따른 데이터들은 *i.i.d* 가 아니다 (서로 연관되어 있을 수도 있다).
- 에이전트 (Agent)의 행동이 그 다음 데이터에 영향을 끼친다.

위에서 언급한 **보상**은 스칼라 형태를 가지는 피드백이라고 볼 수 있는데 에이전트가 시간 $t$ 에서 얼마나 잘 하고 있는지를 알려주는 척도이다. 따라서 에이전트의 목표는 보상의 누적값을 최대화 하는 방향으로 행동을 하게 된다. 다시말해서 보상과 관련된 중요한 정의는 아래와 같다.

> 강화 학습의 모든 목표는 보상 누적값의 기대값을 최대화 하는 것이다.

### Agent, Environment and State
<div class="imgcap">
<img src="/assets/RL/agent_env.png" style="max-height:300px">
<div>[그림1] 에이전트와 환경</div>
</div>
[그림1]은 에이전트와 환경간의 관계를 나타낸다. 매 시간마다 에이전트는 행동 (Action)을 하고, 관측값 (Observation)과 보상값을 얻는다. 반면 환경은 에이전트가 취한 행동을 보고 관측값과 보상값을 발생시킨다. 여기서 **과거** (History)라 용어를 정의 할 수 있는데 과거는 이전의 모든 관측값, 행동 그리고 보상들의 시퀀스를 의미한다 (아래 수식 참고).

$$
H_{t} = O_{1}, R_{1}, A_{1}, ..., A_{t-1}, O_{t}, R_{t}
$$

다음에 어떤 일이 일어날지는 과거에 에이전트가 했던 행동과 환경이 발생시킨 관측값/보상값을 토대로 결정된다. 또한우리는 **상태** (State)라는 용어도 정의할 수 있는데 상태는 다음에 어떤 일이 발생할 지 결정하는데 사용되는 정보이고 $S_{t} = f(H_{t})$ 의 형태로 표현된다 .

**환경 상태** (Environment State) $S_{t}^{e}$ 는 환경이 내부에서 가지고 있는 (private 한) 상태이며 다음번의 관측값/보상값을 결정하기 위해 사용한다. 하지만 대부분의 경우 에이전트가 환경 상태를 직접 볼 수 없으며 볼 수 있다고 하더라도 이 상태에는 필요 없는 정보가 속할 가능성이 크다.

반면 **에이전트 상태** (Agent State) $S_{t}^{a}$ 는 에이전트가 내부에서 가지고 있는 정보를 의미한다. 환경 상태와 비슷하게 에이전트 상태도 에이전트가 다음에 어떤 행동을 할지를 결정할 때 사용되는데 이 상태가 강화 학습 알고리즘에서 주로 사용되는 정보이다. 에이전트 상태는 $S_{t}^{a} = f(H_{t})$ 와 같이 과거의 정보를 이용해서 도출해 낼 수 있다.

**정보 상태** (Information State)는 과거로부터 중요한 모든 정보를 포함하고 있는 상태인데 흔히 *마코프 상태* (Markov State)라고 알려져 있다. 마코프 상태는 간략하게 *"미래는 현재 상태가 주어졌을 때 과거와 독립적이다"* 라는 의미를 갖는 상태이고 수식으로 표현하면 아래와 같다.

$$
\mathbb{P}[S_{t+1} \ | \ S_{t}] = \mathbb{P}[S_{t+1} \ | \ S_{1}, ..., S_{t}]
$$

따라서 정보 상태가 마코프 상태라는 가정 하에 만약 현재 상태를 구했다면 미래 상태를 구하기 위해서 과거의 상태들을 볼 필요가 없게 된다. 이제부터 등장하는 환경 상태와 과거상태 모두 마코프 특성을 따른다.

### Observable
Full observability 는 에이전트가 환경 상태를 직접적으로 관측할 수 있는 경우를 의미인데 이는 $O_{t} = S_{t}^{a} = S_{t}^{e}$ 와 같다. 다시 표현하면 에이전트의 상태는 환경 상태 그리고 정보 상태와 같다. Full observability 는 *Markov Decision Process (MDP)* 라고 불리며 이 강의에서 주로 다루는 내용이기도 하다.

반대로 Partial observability 는 에이전트가 환경을 간접적으로 관측할 수 있는 것을 의미한다. 예를 들어 포커 게임을 할 때 플레이어 (에이전트)는 공개된 카드만 관측할 수 있으며, 다른 플레이어 혹은 딜러가 가지고 있는 카드는 보지 못한다. 따라서 에이전트의 상태와 환경 상태는 같지 않으며 이를 다른 용어로 *Partially Observable Markov Decision Process (POMDP)* 라 부른다.

### RL Components
강화 학습에서의 에이전트는 *정책 (Policy)*, *가치 함수 (Value function)*, *모델 (Model)* 로 구성된다. 모두 포함할 필요는 없으며 이 중 하나라도 포함하면 된다.

**정책**은 에이전트의 행동을 결정하는 역할을 하는데 상태와 행동간의 맵핑 함수라고 볼 수 있다.
또한 결정론적 정책은 $a = \pi(s)$ 와 같이 다음 행동을 직접적으로 결정하는 정책이며, 확률론적 정책은 $\pi(a|s) = P[A_{t} = a | S_{t} = s]$ 와 같이 확률을 통해 다음에 행할 행동을 결정한다.

**가치 함수**는 미래의 보상을 예측하는 함수이다. 때문에 상태의 좋음/나쁨을 평가하는데 사용되며 가치 함수를 통해 나온 평가치를 이용해 다음 행동을 선택하는데 사용한다.

$$
V_{\pi}(s) = \mathbb{E}[R_{t+1} + \gamma R_{t+2} + \gamma^{2} R_{t+3} + ... \ | \ S_{t} = s]
$$

위 수식과 같이 현재 (시간 $t$) 상태의 가치값은 미래에 얻을 수 있는 모든 보상의 누적값과 같다. 이 수식에서 보이는 $\gamma$ 는 discount 인자라고 불리는데 나중에 다시 설명하도록 하겠다.

**모델**은 환경이 다음에 어떤 상태와 보상을 취할지 판단한다.
<div>
\begin{aligned}
P_{ss'}^{a} &= \mathbb{P}[S_{t+1} = s' \ | \ S_{t} = s, A_{t} = a] \\
R_{s}^{a} &= \mathbb{E}[R_{t+1} \ | \ S_{t} = s, A_{t} = a]
\end{aligned}
</div>
$P$ 는 현재 상태와 행동을 보고 다음 상태를 예측하는 함수이며 $R$ 은 현재 상태와 행동을 통해 바로 다음 보상값을 예측하는 역할을 한다. 지금 모델에 대해 자세히 설명하지는 않겠지만 강화 학습에서는 모델을 알고 학습을 하는지, 전혀 알지 못한 상태에서 학습을 하는지에 따라 서로 다른 방법론을 취하게 된다.

### Learning and Planning
위에서 얘기한 것 처럼 모델 그리고 환경을 아는지 모르는지에 따라 다른 방법론을 통해 강화 학습을 진행한다.<br>
여기서 **학습** (Learning)은 초기 환경과 모델을 알 수 없는 경우이며 에이전트는 환경과의 상호작용을 통해 정책을 향상시킨다. 반면 **Planning**은 환경과 모델을 아는 것을 전제로 하여 에이전트는 추가적인 상호작용 없이 알고 있는 모델을 바탕으로 연산을 수행한다. 더 자세한 사항은 나중에 자세히 설명하고자 한다.

### Exploration and Exploitation
결국 강화 학습은 사람이 시행착오를 겪으며 배우는 것과 비슷하게 학습을 해나간다. 이 때 **활용** (Exploitation)과 **탐험** (Exploration)을 기반으로 학습한다. 활용은 알려진 정보를 토대로 보상을 최대화 하는 방법이고 탐험은 환경의 새로운 정보를 찾아나가는 과정이다.<br>
예를 들어 저녁 메뉴를 선택 할 때 기존에 내가 가봤던 음식점 중 가장 좋았던 곳을 가는 것은 활용, 새로운 음식점을 가는 것은 탐험이라고 볼 수 있다. 만약 내가 항상 가봤던 음식점만 간다면 실패할 가능성은 줄지만  더 나은 음식점을 찾을 수 있는 가능성이 없을 것이다. 이와 같이 활용과 탐험을 적절하게 섞는 것이 중요하다.

### Prediction and Control
마지막으로 강화 학습에서 **예측** (Prediction)은 주어진 정책을 사용했을 때의 현재 상태의 가치를 도출한다. 반면 **컨트롤** (Control)은 가능한 모든 정책들을 예측을 통해 가치를 도출해 내고 이 도출된 가치를 통해 가장 좋은 정책을 찾는 문제이다. 따라서 강의에서는 어떻게 정책을 예측할지를 배운 뒤 정책을 컨트롤 할 수 있는 방법을 배운다.

<h1 id="MDP">Markov Decision Process</h1>
---
2강 Markov Decision Process에서는 강화 학습의 근간이 되는 마코프 결정 프로세스에 대해 배운다. 이를 위해 먼저 마코프 성질, 마코프 프로세스, 마코프 보상 프로세스등을 순차적으로 배우고 마지막에 마코프 결정 프로세스를 배우는데 수식이 조금 많다. 하지만 이번 강의에서 배운 수식 (벨만 방정식 패밀리)들을 그 다음 강의에서 사용하기 때문에 어느 정도의 이해는 해야 강의를 듣는데 수월하지 않을까 생각한다.

## Markov Property
상태 $S_{t}$ 가 마코프 성질을 만족하기 위해선

$$
\mathbb{P}[S_{t+1} \ | \ S_{t}] = \mathbb{P}[S_{t+1} \ | \ S_{1}, ..., S_{t}]
$$

여야 한다. 다시 말하면 마코프 성질을 만족하는 어떤 상태는 과거로부터 필요한 모든 정보들을 담고 있기 때문에 현재 상태를 알면 과거의 정보를 알지 못해도 미래 상태를 예측할 수 있다는 의미이다.

## State Transition Matrix
마코프 성질을 만족하는 상태 $s$ 와, 다음 상태 (Successor state) $s'$ 이 있을 때, 상태 전이 확률은 아래와 같이 정의된다.

$$
P_{ss'} = \mathbb{P}[S_{t+1} \ | \ S_{t}]
$$

또한 상태 전이 행렬 $P$ 는 모든 상태 $s$ 와, 다음 상태 $s'$ 을 이용하여 다음과 같이 표현 할 수 있다.

$$
\qquad \qquad \text{to} \\\\
P = \text{from}
\begin{bmatrix}
        P_{11} & \cdots & P_{1n} \\
        \vdots &  & \vdots \\
        P_{n1} & \cdots & P_{nn} \\
\end{bmatrix}
$$

이 때 각 행의 합은 모두 1이다.

## Markov Process
**마코프 프로세스**는 각각의 상태가 모두 마코프 성질을 갖고 있는 상태의 시퀀스를 의미한다. 마코프 프로세스는 마코프 성질에 의해 memoryless 한 특성을 가지고 있다 (과거를 기억하지 않는다).
또한 마코프 프로세스는 $<S, P>$ 와 같은 튜플의 형태로 구성되며 $S$ 는 모든 상태의 집합을, $P$ 는 상태 전이 확률 행렬을 의미한다.
<div class="imgcap">
<img src="/assets/RL/student_MCTM.png">
<div>[그림2] Student Markov Chain Transition Matrix</div>
</div>
[그림2]와 같이 왼쪽의 예시를 상태 전이 행렬의 형태로 나타내면 오른쪽과 같다. 이 예시에서 나타낼 수 있는 **에피소드**의 예시는 C1, C2, C2, Pass, Sleep 등등이 있다.

## Markov Reward Process
**마코프 보상 프로세스**는 마코프 체인에 가치가 붙은 형태를 의미인데 $<S, P, R, \gamma >$ 인 튜플의 형태로 나타낸다. 이 때 $S, P$ 는 마코프 체인에서와 의미이고 추가된 $R$ 은 $R_{s} = \mathbb{E}[R_{t+1} | S_{t} = s]$ 의 형태를 가지는 보상 함수를, $\gamma$ 는 [0, 1] 사이의 값을 가지는 디스카운트 인자를 의미한다.

또한 이 프로세스의 반환값 (Return) $G_{t}$ 는 시간 $t$ 에서부터 디스카운트된 보상들의 총합과 같다.

$$
G_{t} = R_{t+1} + \gamma R_{t+2} + ... = \sum_{k=0}^{\infty} \gamma^{k}R_{t+k+1}
$$

디스카운트 인자 $\gamma \in [0, 1]$ 는 전체 반환값에서 딜레이된 보상과 즉시 얻은 보상과의 관계를 나타내는데 만약 $\gamma$ 가 0에 가깝다면 현재의 보상만 중요시하는 근시안적인 가치 평가가 될 것이며 1에 가깝다면 (디스카운팅을 하지 않는다면) 먼 미래도 내다 보는 가치 평가가 될 것이다.

강의에 따르면 대부분의 마코프 (보상/결정) 프로세스는 디스카운팅을 한다고 한다. 이 이유로는

- 디스카운트 보상을 사용하면 수학적으로 편리하다.
- 만약 마코프 프로세스에 사이클이 있는 경우 디스카운트 보상을 사용하면 반환값이 무한이 되는 것을 막을 수 있다.
- 사람은 즉각적인 보상을 더 선호하긴 하지만 미래의 보상도 염두해 두면서 행동을 한다.
- 예를 들어 모든 시퀀스가 종료 상태인 경우 디스카운트 하지 않은 마코프 보상 프로세스로는 표현이 불가능 하다.

### Value Function
가치 함수 $v(s)$ 는 상태 $s$ 의 가치를 반환한다. 따라서 가치 함수는 에이전트가 현재 상태에서 얼마나 좋은지를 알려주는 척도라고 볼 수 있다. 그리고 *얼마나 좋은지*는 현재 상태에서 기대할 수 있는 미래의 보상값들로 정의할 수 있기 때문에 반환값 $G_{t}$를 통해 표현할 수 있다.

$$
v(s) = \mathbb{E}[G_{t} \ | \ S_{t}=s]
$$

가치 함수와 반환값에 대한 예시는 [그림3]과 같다.
<div class="imgcap">
<img src="/assets/RL/student_MRP.png" style="max-height:400px">
<div>[그림3] Student MRP</div>
</div>
또한 [그림3]의 예시에서  시작 상태 $S1 = C1$, $\gamma = 1/2$ 인 경우를 가정하고 몇가지 샘플 에피소드의 반환값을 계산하면 아래와 같다.
<div class="imgcap">
<img src="/assets/RL/student_MRP_return.png">
<div>[그림4] Student MRP의 반환값</div>
</div>

### Bellman Equation for MRPs
가치 함수는 아래와 같이 두 개의 파트로 나눌 수 있다.

- 즉각적인 (immediate) 보상 $R_{t+1}$
- 다음 상태들 (Successor state)의 디스카운트 된 가치 $\gamma v(S_{t+1})$

위의 사실들을 이용해서 가치 함수를 쪼갤 수 있는데 그 과정은 아래와 같다.
<div>
\begin{aligned}
v(s) &= \mathbb{E}[G_{t} \ | \ S_{t} = s] \\
     &= \mathbb{E}[R_{t+1} + \gamma R_{t+2} + \gamma^2 R_{t+3} + ... \ | \ S_{t} = s] \\
	 &= \mathbb{E}[R_{t+1} + \gamma(R_{t+2} + \gamma R_{t+3} + ...) \ | \ S_{t} = s] \\
	 &= \mathbb{E}[R_{t+1} + \gamma G_{t+1} \ | \ S_{t} = s] \\
	 &= \mathbb{E}[R_{t+1} + \gamma v(S_{t+1}) \ | \ S_{t} = s]
\end{aligned}
</div>
위의 유도에서 마지막 항을 **벨만 방정식** 이라고 부른다. 이 방정식은 $v(s) = \mathbb{E}[R_{t+1} + \gamma v(S_{t+1}) \ | \ S_{t} = s]$ 의 형태로도 사용할 수도 있고 아래와 같은 식으로도 사용 하기도 한다.

$$
v(s) = R_{s} + \gamma\sum_{s' \in S}P_{ss'}v(s')
$$

위 벨만 방정식을 설명하면 가치 함수는 현재 상태 $s$ 의 즉각적인 보상값과 다음 상태 $s'$ 의 보상값을 합한것과 같다는 의미이다 (물론 $\gamma$ 에 따라 디스카운트 여부를 결정한다). 그리고 상태 $s'$의 가치값은 다시 그 다음 상태 $s''$ 와도 연관이 있는 재귀적인 구조를 가진다.

그리고 이 때 상태 전이 확률 행렬 $P$ 에 의해 자동적으로 각 상태들의 기대값을 계산된다. 따라서 가치 함수를 계산하기 위해서는 상태 전이 확률 행렬을 알아야하는데 이 말은 모든 상태들 간의 전이 확률을 알아야 한다는 의미이다. 일반적인 행렬을 사용한다면 상태 전이 확률을 저장하는데는 $O(n^{2})$ 의 공간이 필요하기 때문에 매우매우 비효율적이다. 이와 관련된 내용은 Function Approximation 강의에서 더 자세히 다루기로 하자.

편의성을 위해 벨만 방정식을 행렬의 형태로 나타내면 다음과 같다.

$$
v = R + \gamma Pv
$$

<div class="imgcap">
<img src="/assets/RL/bellman_matrix_form.png" style="max-height:80px">
</div>
이제 MRP에서 각 상태에서의 가치값은 벨만 방정식의 해를 찾으면 구하는 문제로 바뀌었다. MRP에서의 벨만 방정식은 선형 방정식이기 아래와 같이 역행렬을 구해서 직접적으로 구할 수 있다.

$$
v = (1-\gamma P)^{-1}R
$$

역행렬을 이용하여 선형 방정식의 해를 구하면 상태가 $n$ 개 있을 때 $O(n^3)$ 의 시간 복잡도의 성능을 보인다. 이 때문에 상태의 개수가 매우 작은 간단한 MRP 문제만 직접적으로 해를 구할 수 있다. 그래서 만약 MRP가 복잡하다면 (거의 대부분의 경우) 위와 같이 역행렬을 이용해 직접적으로 해를 구하지 않고 *동적 계획법*을 사용하거나 *몬테-카를로 평가법* 등의 반복법 (Iterative method)을 주로 사용한다.

## Markov Decision Process
**마코프 결정 프로세스**는 마코프 보상 프로세스에 *결정*이 추가로 들어간 형태이며 $<S, A, P, R, \gamma>$ 의 튜플과 정책 $\pi$ 로 구성된다. $A$ 는 행동의 집합을 의미하고 나머지는 마코프 보상 프로세스와 같다.

### Policy
정책 $\pi$ 는 에이전트의 상태가 주어졌을 때 다음번에 수행 할 행동들의 확률, 즉 가능성을 나타낸다.

$$
\pi(a \ | \ s) = \mathbb{P}[A_{t} = a \ | \ S_{t} = s]
$$

우리는 정책을 통해 에이전트의 행동을 정의할 수 있는데 MDP에서의 정책은 마코프 성질에 의해서 과거에 영향 받지 않고 오직 현재 상태에만 의존한다. 만약 어떤 MDP에 $M = <S, A, P, R, \gamma>$ 와 정책 $\pi$ 가 주어졌을 때 상태 시퀀스 $S_{1}, S_{2}, ...$ 는 $<S, P^{\pi}>$ 의 형태를 가지는 마코프 프로세스이고 상태와 보상 시퀀스 $S_{1}, R_{2}, S_{2}, ...$ 는 $<S, P^{\pi}, R^{\pi}, \gamma>$ 의 형태인 마코프 보상 프로세스이다. 이 때,
<div>
\begin{aligned}
P_{s, s'}^{\pi} &= \sum_{a \in A}\pi(a|s)P_{ss'}^{a} \\
R_{s}^{\pi}     &= \sum_{a \in A}\pi(a|s)R_{s}^{a} \\
\end{aligned}
</div>
이며, 이 식을 통해 마코프 결정 프로세스에서의 상태 전이 함수와 보상값은 에이전트가 행하는 정책에 의해 변화하는 것을 볼 수 있다.

### Value Function
**상태-가치 함수** $v_{\pi}(s)$ 는 시작 상태 $s$ 에서 정책 $\pi$ 를 가졌을 때 얻을 수 있는 반환값의 기대값을 의미하며 다음과 같이 표현된다. $v_{\pi}(s) = \mathbb{E_{\pi}}[G_{t} \ \| \ S_{t} = s]$ 사실 정책에 의해 가치 함수가 변화 할 수 있다는 점을 제외하면 MRF 에서의 가치 함수와 동일하다.

반면 정책 $\pi$ 를 따르는 **행동-가치 함수** $q_{\pi}(s)$ 정책 $\pi$ 하에 시작 상태 $s$ 에서 행동 $a$ 을 했을 때 얻을 수 있는 반환값의 기대값을 의미하고 다음과 같이 표현된다. $q_{\pi}(s, a) = \mathbb{E_{\pi}}[G_{t} \ \| \ S_{t} = s, A_{t} = a]$ 다시 말해서 에이전트가 정책 $\pi$ 를 따를 때 상태 $s$ 에서 행동 $a$ 를 했을 때 얼마나 결과가 좋을지를 나타내는 것으로 볼 수 있다.

가치 함수라 칭하면 흔히 상태-가치 함수를 의미하기 때문에 다음부터는 상태-가치 함수를 편의상 가치 함수라 부르겠다. 또한 왜인지는 모르겠지만 흔히 행동-가치 함수를 Q 의 표기법으로 나타내기 때문에 Q-value 함수라고 부르기도 한다.

### Bellman Expectation Equation
MRP에서의 벨만 방정식 유도와 비슷하게 MDP에서도 상태-가치 함수와 행동-가치 상태 $s$ 에서의 즉각적인 보상과 다음 상태 $s'$ 의 가치로 분리될 수 있기 때문에 아래와 같이 벨만 기대값 방정식이 유도 된다.
<div>
\begin{aligned}
v_{\pi}(s)    &= \mathbb{E_{\pi}}[R_{t+1} + \gamma v_{\pi}(S_{t+1}) \ | \ S_{t} = s] \\
q_{\pi}(s, a) &= \mathbb{E_{\pi}}[R_{t+1} + \gamma q_{\pi}(S_{t+1}, A_{t+1}) \ | \ S_{t} = s, A_{t} = a] \\
\end{aligned}
</div>
각각의 가치 함수를 벨만 기대값 방정식으로 표현하면 [그림7-1]과 같다. 먼저 상태-가치 함수는 행동-가치 함수의 값을 에이전트의 정책을 이용하여 weighted sum한 것이며, 행동-가치 함수는 현재 상태와 행동의 즉각적인 보상값과 모든 다음 상태의 상태-가치 함수를 상태 전이 확률 행렬을 이용하여 weighted sum한 것이다.
<div class="imgcap">
<img src="/assets/RL/bellman_MDP_tree1.png" style="max-height:250px">
<img src="/assets/RL/bellman_MDP_tree2.png" style="max-height:250px">
<div>[그림7-1] 벨만 기대값 방정식</div>
</div>
[그림7-2]는 [그림7-1]의 구조에서 상태를 한 단계 더 내다보는 구조인데 내부의 식은 약간 복잡하지만 비슷한 형태이다.
<div class="imgcap">
<img src="/assets/RL/bellman_MDP_tree3.png" style="max-height:250px">
<img src="/assets/RL/bellman_MDP_tree4.png" style="max-height:250px">
<div>[그림7-2] 벨만 기대값 방정식</div>
</div>
예시를 하나 들면 [그림8]를 통해 빨간색의 상태의 가치값이 어떻게 계산되는지를 볼 수 있다 (아마 정책은 모두 0.5, 디스카운트 인자는 1인 것 같다).
<div class="imgcap">
<img src="/assets/RL/bellman_MDP_ex1.png" style="max-height:400px">
<div>[그림8] 벨만 기대값 방정식 예시</div>
</div>
7.4의 가치값은 [그림7-2]의 왼쪽 수식을 이용하여 계산 할 수 있다. 0.5는 정책 $\pi$ 를 의미하고 1은 Pub을 가는 행동의 즉각적인 보상을 의미하며 1을 제외한 괄호 안의 나머지 식은 Pub을 가는 행동을 취했을 때 도달 가능한 모든 상태들 $s'$을 상태 전이 행렬을 이용하여 weight sum을 구해 기대값을 구한 것이다. 0.5*10은 동일하나 종료 상태이기 때문에 다음 상태에 해당되는 식은 표현하지 않는다.

MRP처럼 MDP의 경우도 벨만 기대값 방정식을 행렬의 형태로 나타낼 수 있다. $v_{\pi} = R^{\pi} + \gamma P^{\pi}v_{\pi}$ 의 형태이고, 해의 방정식은 $v_{\pi} = (1 - \gamma P^{\pi})^{-1}R^{\pi}$ 이다.

### Optimal Value Function
**최적의 상태-가치 함수**는 $v_{\*}(s) = \max_{\pi}v_{\pi}(s)$ 와 같이 가장 좋은 정책의 상태-가치 함수를 의미한다. 비슷하게 **최적의 행동-가치 함수**는 $q_{\*}(s, a) = \max_{\pi}q_{\pi}(s, a)$ 로 표현된다. 최적의 가치 함수는 MDP에서 가장 최적의 가치를 가지는 정책이 무엇인지 알려주기 때문에 이 함수의 해를 찾게 된다면 MDP에서 최적의 정책을 구하는 문제를 풀 수 있게 된다.

### Optimal Policy
결국 강화 학습을 푼다는 것은 최적의 정책을 찾는 일로 귀결된다. 그렇기 때문에 어떤 정책이 더 나은가? 와 같은 질문에 답을 할 수 있어야 하는데 이는 정책에 순서 (Ordering)를 매길 수 있다면 가능해지는 일이다. 따라서 정책의 부분 순서 (Partial Order)를 정의하면 정책간의 비교는 아래와 같이 이루어진다.

$$
\pi \ge \pi ' \quad \text{if} \quad v_{\pi}(s) \ge v_{\pi '}(s), \ \forall s
$$

그리고 모든 마코프 결정 프로세스에서

- 만약 최적의 정책 $\pi_{\*}$ 가 있으면 이 정책은 다른 정책들 보다 낫거나 같다는 의미이다.,  $\pi_{\*} \ge \pi , \\ \forall \pi$
- 모든 최적의 정책은 최적의 가치 함수를 도출할 수 있다.,  $v_{\pi_{\*}}(s) = v_{\*}(s)$
- 모든 최적의 정책은 최적의 행동-가치 함수를 도출할 수 있다.,  $q_{\pi_{\*}}(s, a) = q_{\*}(s, a)$

그리고 아래 수식과 같이 

$$
\pi_{*}(a|s) = 
\begin{cases}
1  & \text{if $a = \operatorname{argmax}_{a \in A}q_{*}(s, a)$} \\
0  & \text{otherwise}
\end{cases}
$$

최적의 정책은 최적의 행동-가치 함수를 최대화하는 방향으로 찾을 수 있으며 모든 MDP에서 항상 결정론적인 최적의 정책이 존재한다는 특징이 있다.
<div class="imgcap">
<img src="/assets/RL/optimal_policy_ex1.png" style="max-height:400px">
<div>[그림9] 최적의 정책 예시</div>
</div>
그래서 [그림9] 예시 처럼 모든 가능한 행동에 대해 최적의 행동-가치 함수 $q_{\*}$ 를 미리 구해놓은 상태라면 단순히 $q_{\*}$ 이 높은 행동들을 따라 경로를 그려서 바로 최적의 정책을 구할 수 있다.

### Bellman Optimality Equation
MRP에서는 벨만 방정식을, MDP에서는 벨만 기대값 방정식을 알아보았는데 이제 마지막으로 벨만 최적화 방정식을 알아볼 차례이다. 
<div class="imgcap">
<img src="/assets/RL/bellman_optimality_eq1.png" style="max-height:250px">
<img src="/assets/RL/bellman_optimality_eq2.png" style="max-height:250px">
</div>
<br>
<div class="imgcap">
<img src="/assets/RL/bellman_optimality_eq3.png" style="max-height:300px">
<img src="/assets/RL/bellman_optimality_eq4.png" style="max-height:300px">
<div>[그림10] 벨만 최적화 방정식</div>
</div>
벨만 기대값 방정식과 비슷한 구조로 표현하면 [그림10]과 같다. 최적의 행동-가치 함수는 벨만 기대값 방정식에서의 행동-가치 함수와 같으며 최적의 가치 함수는 최적의 행동-가치 함수의 최대값으로 표현하게 된다 (호의 형태로 나타낸 것은 최대값 연산을 의미한다). 

<div class="imgcap">
<img src="/assets/RL/bellman_optimality_eq_ex1.png" style="max-height:400px">
<div>[그림11] 벨만 최적화 방정식 예시</div>
</div>
벨만 최적화 방정식을 이용하여 상태의 최적 가치 함수를 도출하면 [그림11]과 같다. 현재 상태의 최적의 가치 함수는 행동을 할 때 오는 즉각적인 보상과 그 다음 상태의 최적의 가치 함수를 통해서 재귀적으로 구할 수 있으며, 또 구해진 최적의 가치 함수를 이용해서 최적의 상태-가치 함수를 구할 수 있다. [그림10]의 예시에서는 모든 $q_{\*}$ 가 구해진 상태를 가정했지만 이제 벨만 최적화 방정식을 이용하면 $q_{\*}$ 도 직접 구할 수 있는 것이다.

MRP와 마찬가지로 MDP에서도 벨만 방정식을 이용해 가치 함수를 찾을 수 있는 식을 만들었지만 문제는 벨만 최적화 방정식은 비선형 방정식이라는 것에서 차이를 보인다. 일반적으로 비선형방정식은 쉽게 풀 수 없기 때문에 분석적인 방법을 통해 구하기는 어렵고 대신 반복적 방법 예를들어 *가치 반복 (Value Iteration)*, *정책 반복 (Policy Iteration)*, *Q-학습*, *Sarsa*등을 사용한다.

<h1 id="Planning">Planning by Dynamic Programming</h1>
---
1, 2강에서 강화 학습의 근간이 되는 용어들과 MDP에 대해서 공부하였다. 이제 3강부터 어떻게 MDP의 해를 찾을 수 있을지에 대해 공부한다. 3강에서는 에이전트가 모델을 알고 있다는 가정 하에 MDP의 해를 구하는 planning에 대해 알아 본다. 그리고 4, 5강은 모델에 대한 지식이 필요 없는 비-모델 강화 학습을 배우게 된다.

## Dynamic Programming
동적 계획법 (Dynamic Programming)은 복잡한 문제를 풀기 위한 방법이다. 분할-정복법과 비슷하게 복잡한 문제를 여러개의 작은 문제로 쪼갠 뒤 작은 문제를 풀고 합치는 방법을 통해 문제를 해결한다. 아래와 같은 특징을 가지고 있는 문제에서는 동적 계획이 많이 쓰인다.

- 최적의 부분 구조: 최적화의 원칙에 의해 최적의 해는 여러개의 부분 문제로 분할할 수 있다.
- 부분 문제의 반복: 부분 문제를 여러번 구해야 하는 상황이 생기므로 부분 문제의 해를 캐싱해놓고 재사용할 수 있다.

여기서 동적 계획법과 분할-정복법의 차이는 부분 문제의 반복 특징을 적용하느냐에 달려있는데, 반복해서 등장하는 부분 문제를 미리 계산하고 캐싱하기 때문에 원래 문제의 시간 복잡도를 매우 단축할 수 있지만 캐싱 문제에 의해 메모리를 많이 차지하는 단점도 있다.

동적 계획법은 MDP의 모든 정보를 알 경우에 적용할 수 있으며 이는 MDP의 계획법 (Planning)에 사용된다. 이 중 *예측 (Prediction)* 문제는 입력으로 MRP 혹은 정책 $\pi$ 와 MDP를 받으며 이를 이용하여 가치 함수 $v_{\pi}$ 를 도출해낸다. 반면 *컨트롤* 문제에서는 정책을 제외한 MDP를 입력으로 받고 출력은 최적의 가치 함수 $v_{\*}$ 와 최적의 정책 $\pi_{*}$ 를 도출한다.

MDP의 해를 구할 때 강화 학습을 사용하는 이유는 MDP의 해를 구하는 작업이 재귀적으로 일어나기 때문이다. 예를 들어 현재 상태의 가치값은 다음 상태의 가치값을 알아야 하며 다음 상태의 가치값은 다음 다음 상태의 가치값을 알아야 구할 수 있는 것 처럼 말이다. 따라서 일반적인 방법을 사용한다면 모든 값들을 다시 계산하기 때문에 성능이 매우 떨어진다. 그렇기 때문에 어느정도의 메모리를 포기하더라도 동적 계획법을 사용하는 것이다.

## Policy Evaluation
**정책 평가** (Policy Evaluation)는 정책 $\pi$ 가 주어졌을 때 이 정책이 얼마나 좋은지를 평가하는 것이다. 이를 위해서 벨만 기대값 방정식을 동적 계획법을 이용하여 반복적으로 계산하는 방법을 사용한다. 정책 평가를 동기화적 백업 방법 (Synchronous backups)을 사용한다면,

1. 매 이터레이션 $k$ 마다 모든 상태 $s \in S$ 을 순회하며,
2. $v_{k}(s')$ 를 이용해서 $v_{k+1}(s)$ 업데이트 한다 (이 때 $s'$ 는 상태 $s$ 의 다음 상태 (Successor state)를 의미).

와 같다. 증명을 하진 않겠지만 반복적 정책 평가는 잘 수렴 한다고 한다. 그리고 반복적 정책 평가를 수식으로 표현하면,
<div>
\begin{aligned}
v_{k+1}(s) &= \sum_{a \in A}\pi(a \ | \ s)(R_{s}^{a} + \gamma \sum_{s' \in S}P_{ss'}^{a}v_{k}(s')) \\
\boldsymbol{v}^{k+1} &= \boldsymbol{R^{\pi}} + \gamma \boldsymbol{P^{\pi}}\boldsymbol{v}^{k}
\end{aligned}
</div>
인데, 이는 정책 $\pi$ 일 때 $k+1$ 번째 이터레이션의 가치값은 이전 이터레이션의 가치값을 벨만 기대값 방정식에 집어 놓은 형태이다. 따라서 $k+1$ 번째 이터레이션에서의 특정 상태 $s$ 의 가치값은 이전 이터레이션에서의 상태들 $s' \in S$ 에 의해 계산되기 때문에 이 계산 방식을 동적 계획법을 사용하여 구한다면 많은 성능 향상을 이룰 수 있다.<br>
정책 평가를 예제를 통해 알아보기 위해 아래와 같은 간단한 그리드 세계를 생각해보자. 이 그리드 세계는,
<div class="imgcap">
<img src="/assets/RL/policy_eval_grid_world.png" style="max-height:200px">
<div>[그림12] 그리드 세계</div>
</div>

- $\gamma$ 값이 1인 MDP이며, 회색 그리드는 종료 상태를 의미한다.
- 보상은 모두 -1 이다.
- 에이전트는 랜덤 정책을 따른다 (동서남북 각각 방향으로 이동할 확률은 모두 0.25 이다).

<div class="imgcap">
<img src="/assets/RL/policy_eval_result1.png" style="max-height:350px">
&nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp;
<img src="/assets/RL/policy_eval_result2.png" style="max-height:350px">
<div>[그림13] 정책 평가 예시 </div>
</div>
k=0일 때는 모든 상태의 가치값을 0.0으로 초기화 하고, 첫 이터레이션 (k=1)에서는 $\boldsymbol{v}^{k+1} = \boldsymbol{R^{\pi}} + \gamma \boldsymbol{P^{\pi}}\boldsymbol{v}^{k}$ 공식을 이용해서 모든 상태들의 가치값을 업데이트 한다. 예를들어 [0,1] 번재 상태의 업데이트는 -1 + (0.0+0.0+0.0)/3 이기 때문에 -1.0이 나오게 되고 비슷하게 두번재 이터레이션의 [0, 1]번째 상태는 -1 + (0.0-1.0-1.0)/3 이므로 -1.75의 가치값을 가지게 된다. 이 방식으로 계속해서 가치값을 업데이트하게 된다면 점차적으로 수렴하게 된다.

이 예제에서는 이전 이터레이션의 모든 가치값들을 저장해놓고 있었기 때문에 현재 이터레이션에서 가치값을 계산할 때 저장된 상태들의 가치값을 참조하기만 하면 간단하게 계산이 된다. 위에서 언급했지만 이런 성능 향상의 문제 때문에 **모델을 알고 있는 가정하에서** 동적 계획법을 사용해 MDP의 해를 구하는 것이다.

## Policy Iteration
policy improvement theorem 넣기

위의 예시에서는 먼저 정책을 고정시키고 (랜덤 정책) 가치값을 반복적으로 업데이트하면서 수렴해 가는 정책 평가를 통해 정책 $\pi$ 일 때 모든 상태의 가치값들을 구했다. 하지만 단순히 정책을 평가하는데 그치지 않고 정책을 최상의 정책으로 만들기 위해서는 정책 평가를 통해 얻어낸 정책의 가치값들을 이용해 정책을 업데이트 해야한다. 이러한 방식을 **정책 반복** (Policy Iteration)이라 부른다.
<div class="imgcap">
<img src="/assets/RL/policy_eval_result3.png" style="max-height:350px">
<img src="/assets/RL/policy_eval_result4.png" style="max-height:350px">
<div>[그림14] 정책 반복 예시</div>
</div>
정책 반복은 정책 평가를 통해 도출한 상태의 가치값을 이용해 **그리디**한 방식으로 정책을 업데이트 한다. 위 예시는 정책을 랜덤하게 초기화를 했기 때문에 k=0 인 경우 랜덤 정책을 가지고 있다. 그리고 k=1 인 이터레이션에서 정책 평가를 통해 구해진 현재 정책하에서의 가치값을 이용해 현재 갈 수 있는 상태 중 가장 높은 가치값을 가지고 있는 상태로 행동을 할 수 있도록 정책을 수정한다.

이 예제는 매우 작은 예제이기 때문에 k=3 이후부터 정책이 최적화된 정책으로 수렴하는 것을 볼 수 있다. 하지만 실제 세계 혹은 복잡한 세계에서는 더 많은 이터레이션이 필요로 하다. 또한 정책 평가와 마찬가지로 정책 반복도 최적화된 정책으로 수렴한다는 것으로 알려져 있다.

다시 정리하면 정책 반복은 아래 의사코드와 같다.
<div class="imgcap">
<img src="/assets/RL/policy_iter.png" style="max-height:400px">
<div>[그림15] 정책 반복 의사 코드</div>
</div>
먼저 정책과 가치값을 임의로 초기화한다. [그림14]의 예제에서는 초기 가치값은 모두 0.0으로, 정책은 랜덤 정책으로 초기화 하였다.
그 후 정책에 따라 가치값을 평가한다 (정책 평가). 벨만 기대값 방정식을 이용해서 모든 상태의 가치값이 수렴할 때 까지 가치값을 업데이트 한다.
이제 가치값이 수렴했으면 이 가치값을 통해 정책을 업데이트 한다. 정책을 향상시키는 것은 정책 평가를 통해 구한 가치값을 이용해서 그리디한 방식으로 가능하다. 만일 업데이트 된 정책을 이전 이터레이션과 비교해 수렴한다면 알고리즘을 종료하게 된다.

## Value Iteration
작성중

<h1 id="Model-Free">Model-Free Prediction and Control</h1>
---
위에서 설명한 동적 계획법을 사용한 Planning 방법은 MDP를 풀기 위해서 모델이 어떻게 생겼는지 알고있어야 한다. 하지만 대부분의 현실 세계에서는 MDP 모델을 알지 못하거나 혹은 모델이 너무 크기 때문에 동적 계획법 방법으로 풀 수 없는 경우가 많다. 그래서 전체를 훑어서 가치값을 계산하는 방법 대신 일부분만 샘플링 한 뒤 샘플링 한 결과를 토대로 해를 구하는 **비모델**(Model-Free) 방법을 많이 사용한다.

이 강의에서는 먼저 모델을 알지 못하는 상황에서 정책에 대한 가치값을 구하는 *비모델 예측*을 배우고 그 후 정책을 변화시켜 가치값을 최적화 하는 *비모델 컨트롤* 방법에 대해 배운다.

## Monte-Carlo Learning
**몬테 카를로 학습법**은 비모델 예측 방법 중 하나이다. 비모델 방법이기 때문에 MDP 모델에 대한 사전지식 없이 해를 구할 수 있으며 이 말은 MDP의 상태 전이 정보나 보상값들을 알지 못해도 사용 가능하다는 의미이다.

정책을 평가하는 방법에 대해 복습을 해보자. 에이전트가 정책 $\pi$ 를 따를 때 $S_{1}, A_{1}, R_{2}, \dots , S_{k} \sim \pi$ 인 에피소드로부터 가치값 $V_{\pi}$ 를 구하는게 목표라고 가정하자. 이 때 반환값은 아래와 같고,

$$
G_{t} = R_{t+1} + \gamma R_{t+2} + \dots + \gamma^{T-1}R_{T}
$$

가치값은 반환값의 기대값과 같다.

$$
v_{\pi}(s) = \mathbb{E}_{\pi}[G_{t} \ | \ S_{t} = s]
$$

일반적인 planning에서는 모든 모델에 대한 정보를 알 수 있었기 때문에 가치값 $v_{\pi}$ 를 **반환값의 기대값**으로 구했지만 비모델 방법에서는 모델에 관한 정보를 알 수 없어 샘플링된 에피소드의 샘플 평균으로 부터 가치값을 구한다. 결국 몬테 카를로 방법의 핵심 아이디어는 가치값을 구할 때 샘플 에피소드의 반환값을 평균을 내어 사용한다는 점에서 차이를 보이는 것이다.

### First-Visit and Every-Visit Monte-Carlo
몬테 카를로 정책 평가는 First-Visit 방법과 Every-Visit 방법으로 나눌 수 있다.<br>
먼저 First-Visit 방법은 상태 s 에서의 가치값을 구하기 위해, 에피소드에서 상태 s 에 방문한 **첫** 순간만 카운터 $N(s)$ 를 1 만큼, 총 반환값 $S(s)$ 를 $G_{t}$ 만큼 증가시킨다. 따라서 가치값은 반환값의 샘플 평균 $V(s) = \frac{S(s)}{N(s)}$ 로 계산될 수 있으며 큰 수의 법칙에 따라 

$$
V(s) \rightarrow v_{\pi} \quad \text{as} \quad N(s) \rightarrow \infty
$$

$V(s)$ 는 가치값으로 수렴하게 된다.

Every-Visit 방법은 나머지는 First-Visit과 동일하지만 상태 s 를 첫 방문시만 카운트를 증가하는게 아닌, 에피소드에서 이 상태를 매 방문 할 때마다 카운트를 증가시킨다. First-Visit과 마찬가지로 큰 수의 법칙에 따라 $V(s)$ 는 가치값으로 수렴한다.

### Incremental Mean
그럼 샘플된 에피소드의 평균은 어떤 방식으로 구해야 할까? 에피소드 $X_{1}, X{2}, \dots$ 가 있다고 가정할 때 가장 단순한 형태는 다음과 같이

$$
\mu_{k} = \frac{1}{k}\sum_{j=1}^{k}X_{j}
$$

구하는 것이다. 이 식은 k 번째 에피소드 까지의 평균을 구하기 위해 이전의 모든 에피소드의 반환값 정보를 저장하고 있어야 한다는 문제가 있다. 따라서 Incremental Mean 이라는 방법을 통해 이를 해결한다. 이 방법은 평균 식을 아래와 같이 변형 하여 식을 도출할 수 있다.
<div>
\begin{aligned}
\mu_{k} &= \frac{1}{k}\sum_{j=1}^{k}X_{j} \\
		&= \frac{1}{k}(X_{k} + \sum_{j=1}^{k-1}X_{j}) \\
		&= \frac{1}{k}(X_{k} + (k-1)\mu_{k-1}) \\
		&= \mu_{k-1} + \frac{1}{k}(X_{k} - \mu_{k-1})
\end{aligned}
</div>
에피소드가 끝나면 모든 상태 $S_{t}$ 에 대해 반환값 $G_{t}$ 를 구할 수 있고, 도출된 위 식을 통해 카운터를 증가 시키고 증가된 카운터를 이용해서 반환값의 샘플 평균을 구한다.
<div>
\begin{aligned}
N(S_{t}) &\leftarrow N(S_{t}) + 1 \\
V(S_{t}) &\leftarrow V(S_{t}) + \frac{1}{N(S_{t})}(G_{t} - V(S_{t}))
\end{aligned}
</div>
이 방식을 사용하면 평균을 구할 때 카운터 $N(S_{t})$ 와 현재 에피소드의 반환값 $G_{t}$ 만 알면 되기 때문에 공간적인 측면에서 매우 효과적이다.
만약 과거의 에피소드를 조금씩 잊고 대신 최근의 에피소드를 많이 반영하고 싶다면 다음과 같이 **이동 평균** 방식을 사용할 수 있다. 

$$
V(S_{t}) \leftarrow V(S_{t}) + \alpha(G_{t} - V(S_{t}))
$$

Batch Normalization과 같은 테크닉에서도 이동 평균을 사용하는데 특히 지수 이동 평균은 매우 많이 사용되는 방식 중 하나이다.

### Summary of MC

- 비모델 방법으로 DP 방식과 달리 MDP 모델에 대한 정보 (상태 전이, 보상..)가 없어도 해를 구할 수 있다.
- 아이디어가 간단하다 (기대값 대신 샘플 에피소드의 반환값의 평균을 구한다는 아이디어).
- 에피소드 전체 시퀀스를 보고 학습한다 (부트스트랩을 사용하지 않는다).
- **단점**: 에피소드가 있는 MDP에만 적용이 가능하고 에피소드가 끝나지 않으면 적용이 불가능하다.

MC 방법은 에피소드의 최종 반환값을 보고 가치값을 업데이트 하기 때문에 에피소드가 끝나지 않는다면 이 방법을 사용하지 못한다는 매우 큰 단점을 가지고 있다.

## Temporal-Difference Learning
Temporal-Difference 방법 (**TD**)은 몬테 카를로 방법과 마찬가지로 MDP의 모델을 알지 못하더라도 적용 가능한 방법이다. 하지만 몬테 카를로와 다르게 에피소드가 끝나지 않는, 불완전한 에피소드여도 *부트스트랩* 방법을 사용하여 학습을 할 수 있다는 장점이 있다.

에피소드로부터 온라인으로 $v_{\pi}$ 를 학습하기 위해 MC 방법은

$$
V(S_{t}) \leftarrow V(S_{t}) + \alpha(\color{red}{G_{t}} - V(S_{t}))
$$

와 같이 $V(S_{t})$ 를 최종 반환값 $G_{t}$ 의 평균 (Incremental mean)을 취해서 계산한다.
반면 TD 방법, 그중에서 가장 간단한 TD(0)는 $V(S_{t})$ 를 *최종 반환값*이 아닌 **예측한 반환값** $R_{t+1} + \gamma V(S_{t+1})$ 를 이용해서 아래와 같이 계산한다 (다음 언급이 없을 때 까지 TD와 TD(0)은 같다고 가정하자).

$$
V(S_{t}) \leftarrow V(S_{t}) + \alpha(\color{red}{R_{t+1} + \gamma V(S_{t+1})} - V(S_{t}))
$$

예측한 반환값 $R_{t+1} + \gamma V(S_{t+1})$ 은 즉각적인 보상값과 다음 스텝에서 얻을 수 있는 디스카운트 된 가치값으로 구성되는데 흔히 이를 **TD 타겟**이라 부른다. 또한 TD 타겟에서 실제 가치값을 뺀 $R_{t+1} + \gamma V(S_{t+1}) - V(S_{t})$ 는 **TD 에러** 라고 한다.
<div class="imgcap">
<img src="/assets/RL/driving_home.png" style="max-height:250px">
<div>[그림15] Driving Home 예제</div>
</div>
[그림15]의 예제는 회사에서 집까지 가는 길을 에피소드화 한 것이다. 사무실을 떠날 때는 집까지 30분정도 걸릴 것으로 예상했지만 비가 와서 차까지 도착하는데 시간이 지체되어 도착시간을 40분으로 늘리고, 그 이후 상황에서도 예상 도착시간이 계속 바뀌는 것을 볼 수 있다.
<div class="imgcap">
<img src="/assets/RL/driving_home2.png" style="max-height:300px">
<div>[그림16] Driving Home 예제</div>
</div>
이 에피소드를 MC와 TD에 학습시키면 [그림16]과 같다. MC는 에피소드가 끝난 후 최종 결과값 (time=43)을 보고 상태들을 업데이트 한다. 따라서 43분보다 적게 예상한 상태들의 예상치들이 모두 올라가게 된다.
반대로 TD 학습법은 다음 상태와 현재 상태의 차이를 통해 현재 상태의 가치값을 학습한다. 때문에 *Temporal-Difference*라는 이름이 붙은 것이다.

이러한 특성 때문에 TD 학습법은 아래와 같은 장점이 있다.

- 매 스텝마다 온라인으로 학습 할 수 있다.
(MC는 에피소드가 끝날 때 까지 기다려야만 한다)
- 마찬가지로 TD는 불완전한 (끝나지 않는) 에피소드를 통해서 학습이 가능하다.
따라서 TD는 에피소드 형식이 아닌 종결이 없는 연속적인 형태의 환경에서도 사용할 수 있다.

만약 자동차 운전을 하다 가까스로 다른 차와 충돌을 피했다고 가정해보자. MC의 경우 최종 반환값만 보고 상태들을 업데이트 하기 때문에 사고가 날 뻔했지만 그에 대한 부정적 피드백을 전혀 받지 못한다. 하지만 TD는 사고가 나기 직전 스텝에서의 가치값에 따라 이전 상태들이 업데이트 되기 때문에 실제로 사고가 나지 않았더라도 이에 맞는 피드백을 받을 수 있다.

혹은 하나의 에피소드가 긍정적인 과정과 부정적인 과정이 섞여서 진행되었고 최종 결과는 부정적이었다면 (차량에 충돌) MC는 에피소드의 모든 상태들이 부정적인 피드백을 받지만 TD는 각자 상태가 그 다음 스텝에 의해 영향을 받기 때문에 서로 다른 피드백을 받을 수 있다는 장점이 있다.

### Bias/Variance Trade-off
MC에서 사용하는 반환값 $G_{t} = R_{t+1} + \gamma R_{t+2} + \dots + \gamma^{T-1} R_{T}$ 는 $v_{\pi}(S_{t})$ 에 대해서 unbiased하다. 추정치가 unbiased한 것은 샘플을 통해서 모집단의 통계량을 정확하게 측정 할 수 있다는 의미이다. **True** TD 타겟 $R_{t+1} + \gamma \color{red}{v_{\pi}}(S_{t+1})$ 도 역시 unbiase한 추정치 이지만 우리가 TD에서 살펴본 TD 타겟 $R_{t+1} + \gamma \color{red}{V}(S_{t+1})$ 은 biased 하다. 이는 TD 타겟에 사용한 $V(S_{t})$ 는 우리가 임의로 가정한 것이기 때문에 초기화 전략에 따라 다르고 실제 값을 대변할 수 있다는 보장이 없기 때문이다.

하지만 TD 타겟은 MC에서 사용하는 반환값보다 Variance가 매우 작다. 이는 MC의 반환값은 에피소드가 끝난 후 결정되기 때문에 현재 스텝에서 에피소드의 반환값 까지의 모든 랜덤한 행동, 보상 등에 종속되어 있는 반면 TD 타겟은 그 다음 스텝에만 종속적이기 때문이다.

그래서 MC는 높은 variance, 낮은 bias를 가지며

- 함수 근사 (function approximation)을 사용하더라도 수렴이 잘된다.
- 초기값으로부터 부트스트랩을 하지 않기 때문에 민감하지 않다.

반면 TD는 낮은 variance, 높은 bias를 가지며

- MC 보다 효과적인 경우가 많다.
- TD는 $v_{\pi}(s)$ 에 수렴하지만 함수 근사를 사용하는 경우 보장되지 않는다.
- 초기값에 민감하다.

라는 특성을 가진다.

### MC and TD
예를들어 상태 A, B가 있고 디스카운트가 없는 모델에서 아래와 같이 에피소드가 진행되었다고 생각해보자.
<div class="imgcap">
<img src="/assets/RL/AB_ex.png" style="max-height:200px">
</div>
이 때 V(A), V(B)의 값은 무엇일까? (0, 1은 보상값이다)
<div class="imgcap">
<img src="/assets/RL/AB_ex2.png" style="max-height:200px">
<div>[그림17] AB 예제</div>
</div>
V(B)값은 8번 중 6번이 1, 2번이 0이기 때문에 대부분 0.75라고 답하겠지만 V(A)값은 사람마다 의견이 다를 수있을 것이다. V(A)를 0으로 답한 경우는 A 상태가 나온 에피소드는 모두 0이기 때문에 0으로 답했을 것이며, 0.75로 답한 경우는 [그림17]의 오른쪽 그래프와 같이 생각했을 것이다. 사실 V(A)의 값을 어떻게 답하냐가 MC와 TD의 차이라고 볼 수 있다. V(A)=0 인 경우는 MC 이고, V(A)=0.75 인 경우는 TD 이다. 

이러한 차이는 MC와 TD의 특성이 다르기 때문에 생긴다.<br>
MC는 **MSE를 최소화** 하는 방향으로 수렴하려고 하며, 이는 아래 수식과 같이 관측된 최종 반환값과 에피소드의 모든 타입스텝과의 차이를 줄이려는 경향으로 학습을 한다.

$$
\sum_{k=1}^{K}\sum_{t=1}^{T_{k}}(G_{t}^{k} - V(S_{t}^{k}))^{2}
$$

예를들어 위 예시에서 V(A)=0 으로 답한 경우는 [A, 0, B, 0] 에피소드에서 MSE를 줄이는 방향으로 생각한 것으로 볼 수 있다.

반대로 TD는 최대 가능도 마코프 모델 (Maximum Liklihood Markov Model)에 수렴한다. 이 말은 TD 학습은 데이터를 보고 그에 맞는 MDP 모델을 만든 후 MDP 모델에 최대 가능도인 답을 찾는 방향으로 학습을 해 나간다는 의미이다.
<div>
\begin{aligned}
\hat{P}_{s, s'}^{a} &= \frac{1}{N(s, a)}\sum_{k=1}^{K}\sum_{t=1}^{T_{k}}\boldsymbol{1}(s_{t}^{k}, a_{t}^{k}, s_{t+1}^{k} \ = \ s, a, s') \\
\hat{R}_{s}^{a} &= \frac{1}{N(s, a)}\sum_{k=1}^{K}\sum_{t=1}^{T_{k}}\boldsymbol{1}(s_{t}^{k}, a_{t}^{k} \ = \ s, a)r_{t}^{k}
\end{aligned}
</div>

종합하자면 TD는 마코프 성질을 이용하기 때문에 보통 마코프 환경인 경우에 적용하면 효과적이다. 반면 MC는 마코프 성질을 이용하지 않고 학습을 하기 때문에 비 마코프 환경에서 효과적이다.
<div class="imgcap">
<img src="/assets/RL/mc_backup.png" style="max-height:250px">
<img src="/assets/RL/td_backup.png" style="max-height:250px">
</div>
<div class="imgcap">
<img src="/assets/RL/dp_backup.png" style="max-height:250px">
<div>[그림18] MC, TD, DP의 backups</div>
</div>
[그림18]는 차례대로 MC, TD, DP의 backups를 나타낸 것이다.<br>
MC는 현재 상태에서 에피소드가 종료할 때 까지의 모든 경로를 보기 때문에 위에서 설명한 것 처럼 비 마코프인 성질을 갖는다. 반대로 TD는 MC와 마찬가지로 에피소드를 샘플링 하지만 바로 다음 스텝만을 보는 부트스트랩 방법을 사용해서 값을 업데이트 한다. 마지막으로 DP의 경우는 TD(0)와 같이 다음 상태만 보는 마코프 성질을 가지고 있지만 MC, TD와 다르게 전체를 보고 값을 업데이트 하는 planning 방법이다.
<div class="imgcap">
<img src="/assets/RL/unified_view.png" style="max-height:350px">
<div>[그림19] Unified view of RL</div>
</div>
또한 지금까지 배운 DP, MC, TD를 [그림19]과 같이 부트스트랩과 샘플링의 통합된 관점에서 볼 수도 있다.

## n-Step TD
지금까지 배운 TD는 정확하게 말하면 TD(0)를 의미한다. TD(0)는 바로 다음 미래만을 보고 현재 스텝의 상태를 업데이트 하는 방식이고 그렇기 때문에 MDP를 만족한다. 그렇다면 더 먼 미래를 보고 현재 스텝의 상태를 업데이트 할 수는 없을까? 이에 대한 해답이 바로 **n-step TD** 이다.
<div class="imgcap">
<img src="/assets/RL/nstep_td.png" style="max-height:300px">
<div>[그림20] n-Step TD</div>
</div>
만약 TD가 스텝 1개만 바라보면 이는 TD(0)와 같으며 (왜 0인지는 후에 설명하겠다), 스텝이 점점 커질수록 MC 방법과 비슷해진다. 이를 수식으로 표현을 하면 아래와 같다.
<div class="imgcap">
<img src="/assets/RL/nstep_return.png" style="max-height:200px">
</div>
위 식을 일반화 하면 반환값은 다음과 같다.

$$
G_{t}^{(n)} = R_{t+1} + \gamma R_{t+2} + \dots + \gamma^{n-1}R_{t+n} + \gamma^{n}V(S_{t+n})
$$

그리고 이 반환값 $G_{t}^{(n)}$ 을 이용하면 일반화 된 n-Step TD 학습법이 된다.

$$
V(S_{t}) \leftarrow V(S_{t}) + \alpha(G_{t}^{(n)} - V(S_{t}))
$$

n-Step TD는 n 값을 통해 TD(0)부터 MC까지 미래를 어느정도까지 보고 상태를 예측할지를 자유자재로 정할 수 있지만 우리가 직접 n 값을 정해주어야 한다는 단점도 가지고 있다. 어떤 모델에서 특정 n 값이 가장 좋은 성능을 내더라도 다른 모델에 적용시키면 다시 최적의 n 값을 설정해야만 한다.

## TD(λ)
n-Step의 단점을 해결하기 위해 여러 n 값에서 나온 반환값을 종합하여 최종 n-Step의 반환값으로 결정 하면 되지 않을까? 예를 들어 2-step과 4-step에서 나온 반환값 $G^{(2)}, G^{(4)}$ 가 있을 때,

$$
\frac{1}{2}G^{(2)} + \frac{1}{2}G^{(4)}
$$

와 같이 평균을 내어 n-Step의 반환값으로 정할 수 있다.<br>
이렇게 여러 스텝의 반환값들을 평균을 내면 다양한 스텝들이 가지고 있는 정보들을 종합할 수 있기 때문에 더 강인한 성능을 보인다. 그렇다면 모든 시간 스텝의 정보들을 종합하는 방법도 생각할 수 있을 것이다. 하지만 MC에서 보았듯이 단순하게 반환값들을 위와 같이 평균을 내는 방식은 모든 반환값을 저장해야만 한다. MC에서는 이를 Incremental Mean 방식을 통해 해결했는데 n-Step TD의 경우 **λ-return** 이라는 방법을 사용한다.
<div class="imgcap">
<img src="/assets/RL/lambda_return.png" style="max-height:300px">
<div>[그림21] λ-return</div>
</div>
λ-return $G_{t}^{\lambda}$ 는 모든 스텝의 반환값들을 종합한 것이다. 종합할 때 사용하는 평균은 단순한 산술 평균이 아닌 일종의 가중치 합을 사용하는데, 가중치 $(1-\lambda)\lambda^{n-1}$ 를 사용해 아래와 같이 계산 한다.

$$
G_{t}^{\lambda} = (1-\lambda)\sum_{n=1}^{\infty}\lambda^{n-1}G_{t}^{(n)}
$$

이 때 앞에 붙은 $(1-\lambda)$ 는 가중치 합을 1로 만들어 평균처럼 사용할 수 있도록 만들어 주는 역할을 한다 (시그마를 풀면 등비수열에 의해 $(1-\lambda)$ 의 역수가 나온다). **TD(λ)** 는 λ-return을 TD-타겟으로 사용하는 TD 방법이다. TD(λ)는 계산을 어떻게 진행하느냐에 의해 Forward-view TD(λ)와 Backward-view TD(λ)로 나뉘는데 Forward-view는 아래와 같다.

$$
V(S_{t}) \leftarrow V(S_{t}) + \alpha(G_{t}^{\lambda} - V(S_{t}))
$$

수식의 형태는 MC, TD와 비슷하다. 대신 최종 반환값 혹은 TD(0)에서 사용한 식 대신 가중치 합을 통해 구한 모든 스텝의 반환값 평균을 TD-타겟으로 사용하는 것이다.

<div class="imgcap">
<img src="/assets/RL/td_lambda_func.png" style="max-height:300px">
<div>[그림22] TD(λ) 가중치 함수</div>
</div>
TD(λ)를 시각화 하면 [그림22]와 같다. 바로 다음 미래는 $(1-\lambda)$ 의 가중치를 갖지만 먼 미래로 갈수록 점차 $\lambda$ 만큼 decay 된다. 

### Forward-view, Backward-view of TD(λ)
TD(λ)는 위에서 설명한 것처럼 계산 방법에 따라 Forward-view와 Backward-view로 나뉜다. 그 중 Forward-view는 가치 함수를 λ-return $G_{t}^{\lambda}$ 의 방향으로 업데이트 하는 방식이다.
<div class="imgcap">
<img src="/assets/RL/forward_td.png" style="max-height:300px">
<div>[그림23] Forward-view TD(λ)</div>
</div>
우리가 이전에 배웠던 TD(0)를 계산하는 방법도 Forward-view와 비슷하다. 하지만 TD(0)은 바로 다음 미래만 보고 

# Reference
[UCL Course on RL by David Silver](http://www0.cs.ucl.ac.uk/staff/d.silver/web/Teaching.html)<br>
[Reinforcement Learning: An Introduction by 김태훈 (carpedm20)](http://www.slideshare.net/carpedm20/reinforcement-learning-an-introduction-64037079)







