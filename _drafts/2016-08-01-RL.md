---
layout: "post"
title: "RL"
excerpt: "RL"
date: "2016-08-01 05:00:00"
---

## Table of contents

- [Introduction](#Intro)
- [Markov Decision Process](#MDP)
- [Planning by Dynamic Program](#Planning)

<h1 id="Intro">Introduction</h1>
---
머신 러닝은 크게 지도 학습과 비지도 학습 두 가지로 분류를 하곤 하는데 강화 학습은 두 갈래 모두에도 속하지 않는 머신 러닝 방법이다. 강화 학습이 다른 머신 러닝 방법과 다른 점은,

- 지도 (Supervisor)가 있는 지도 학습과 달리 **보상** (Reward)만 존재 한다.
- 결정 혹은 행동에 따른 피드백이 즉각적으로 오지 않는다.
- 대부분 시간과 관련되어 있으며, 시간에 따른 데이터들은 *i.i.d* 가 아니다 (서로 연관되어 있을 수도 있다).
- 에이전트 (Agent)의 행동이 그 다음 데이터에 영향을 끼친다.

위에서 언급한 **보상**은 스칼라 형태를 가지는 피드백이라고 볼 수 있는데 에이전트가 시간 $t$ 에서 얼마나 잘 하고 있는지를 알려주는 척도이다. 따라서 에이전트의 목표는 보상의 누적값을 최대화 하는 방향으로 행동을 하게 된다. 다시말해서 보상과 관련된 중요한 정의는 아래와 같다.

> 강화 학습의 모든 목표는 보상 누적값의 기대값을 최대화 하는 것이다.

### Agent, Environment and State
<div class="imgcap">
<img src="/assets/RL/agent_env.png" style="max-height:300px">
<div>[그림1] 에이전트와 환경</div>
</div>
[그림1]은 에이전트와 환경간의 관계를 나타낸다. 매 시간마다 에이전트는 행동 (Action)을 하고, 관측값 (Observation)과 보상값을 얻는다. 반면 환경은 에이전트가 취한 행동을 보고 관측값과 보상값을 발생시킨다. 여기서 **과거** (History)라 용어를 정의 할 수 있는데 과거는 이전의 모든 관측값, 행동 그리고 보상들의 시퀀스를 의미한다 (아래 수식 참고).

$$
H_{t} = O_{1}, R_{1}, A_{1}, ..., A_{t-1}, O_{t}, R_{t}
$$

다음에 어떤 일이 일어날지는 과거에 에이전트가 했던 행동과 환경이 발생시킨 관측값/보상값을 토대로 결정된다. 또한우리는 **상태** (State)라는 용어도 정의할 수 있는데 상태는 다음에 어떤 일이 발생할 지 결정하는데 사용되는 정보이고 $S_{t} = f(H_{t})$ 의 형태로 표현된다 .

**환경 상태** (Environment State) $S_{t}^{e}$ 는 환경이 내부에서 가지고 있는 (private 한) 상태이며 다음번의 관측값/보상값을 결정하기 위해 사용한다. 하지만 대부분의 경우 에이전트가 환경 상태를 직접 볼 수 없으며 볼 수 있다고 하더라도 이 상태에는 필요 없는 정보가 속할 가능성이 크다.

반면 **에이전트 상태** (Agent State) $S_{t}^{a}$ 는 에이전트가 내부에서 가지고 있는 정보를 의미한다. 환경 상태와 비슷하게 에이전트 상태도 에이전트가 다음에 어떤 행동을 할지를 결정할 때 사용되는데 이 상태가 강화 학습 알고리즘에서 주로 사용되는 정보이다. 에이전트 상태는 $S_{t}^{a} = f(H_{t})$ 와 같이 과거의 정보를 이용해서 도출해 낼 수 있다.

**정보 상태** (Information State)는 과거로부터 중요한 모든 정보를 포함하고 있는 상태인데 흔히 *마코프 상태* (Markov State)라고 알려져 있다. 마코프 상태는 간략하게 *"미래는 현재 상태가 주어졌을 때 과거와 독립적이다"* 라는 의미를 갖는 상태이고 수식으로 표현하면 아래와 같다.

$$
\mathbb{P}[S_{t+1} \ | \ S_{t}] = \mathbb{P}[S_{t+1} \ | \ S_{1}, ..., S_{t}]
$$

따라서 정보 상태가 마코프 상태라는 가정 하에 만약 현재 상태를 구했다면 미래 상태를 구하기 위해서 과거의 상태들을 볼 필요가 없게 된다. 이제부터 등장하는 환경 상태와 과거상태 모두 마코프 특성을 따른다.

### Observable
Full observability 는 에이전트가 환경 상태를 직접적으로 관측할 수 있는 경우를 의미인데 이는 $O_{t} = S_{t}^{a} = S_{t}^{e}$ 와 같다. 다시 표현하면 에이전트의 상태는 환경 상태 그리고 정보 상태와 같다. Full observability 는 *Markov Decision Process (MDP)* 라고 불리며 이 강의에서 주로 다루는 내용이기도 하다.

반대로 Partial observability 는 에이전트가 환경을 간접적으로 관측할 수 있는 것을 의미한다. 예를 들어 포커 게임을 할 때 플레이어 (에이전트)는 공개된 카드만 관측할 수 있으며, 다른 플레이어 혹은 딜러가 가지고 있는 카드는 보지 못한다. 따라서 에이전트의 상태와 환경 상태는 같지 않으며 이를 다른 용어로 *Partially Observable Markov Decision Process (POMDP)* 라 부른다.

### RL Components
강화 학습에서의 에이전트는 *정책 (Policy)*, *가치 함수 (Value function)*, *모델 (Model)* 로 구성된다. 모두 포함할 필요는 없으며 이 중 하나라도 포함하면 된다.

**정책**은 에이전트의 행동을 결정하는 역할을 하는데 상태와 행동간의 맵핑 함수라고 볼 수 있다.
또한 결정론적 정책은 $a = \pi(s)$ 와 같이 다음 행동을 직접적으로 결정하는 정책이며, 확률론적 정책은 $\pi(a|s) = P[A_{t} = a | S_{t} = s]$ 와 같이 확률을 통해 다음에 행할 행동을 결정한다.

**가치 함수**는 미래의 보상을 예측하는 함수이다. 때문에 상태의 좋음/나쁨을 평가하는데 사용되며 가치 함수를 통해 나온 평가치를 이용해 다음 행동을 선택하는데 사용한다.

$$
V_{\pi}(s) = \mathbb{E}[R_{t+1} + \gamma R_{t+2} + \gamma^{2} R_{t+3} + ... \ | \ S_{t} = s]
$$

위 수식과 같이 현재 (시간 $t$) 상태의 가치값은 미래에 얻을 수 있는 모든 보상의 누적값과 같다. 이 수식에서 보이는 $\gamma$ 는 discount 인자라고 불리는데 나중에 다시 설명하도록 하겠다.

**모델**은 환경이 다음에 어떤 상태와 보상을 취할지 판단한다.
<div>
\begin{aligned}
P_{ss'}^{a} &= \mathbb{P}[S_{t+1} = s' \ | \ S_{t} = s, A_{t} = a] \\
R_{s}^{a} &= \mathbb{E}[R_{t+1} \ | \ S_{t} = s, A_{t} = a]
\end{aligned}
</div>
$P$ 는 현재 상태와 행동을 보고 다음 상태를 예측하는 함수이며 $R$ 은 현재 상태와 행동을 통해 바로 다음 보상값을 예측하는 역할을 한다. 지금 모델에 대해 자세히 설명하지는 않겠지만 강화 학습에서는 모델을 알고 학습을 하는지, 전혀 알지 못한 상태에서 학습을 하는지에 따라 서로 다른 방법론을 취하게 된다.

### Learning and Planning
위에서 얘기한 것 처럼 모델 그리고 환경을 아는지 모르는지에 따라 다른 방법론을 통해 강화 학습을 진행한다.<br>
여기서 **학습** (Learning)은 초기 환경과 모델을 알 수 없는 경우이며 에이전트는 환경과의 상호작용을 통해 정책을 향상시킨다. 반면 **Planning**은 환경과 모델을 아는 것을 전제로 하여 에이전트는 추가적인 상호작용 없이 알고 있는 모델을 바탕으로 연산을 수행한다. 더 자세한 사항은 나중에 자세히 설명하고자 한다.

### Exploration and Exploitation
결국 강화 학습은 사람이 시행착오를 겪으며 배우는 것과 비슷하게 학습을 해나간다. 이 때 **활용** (Exploitation)과 **탐험** (Exploration)을 기반으로 학습한다. 활용은 알려진 정보를 토대로 보상을 최대화 하는 방법이고 탐험은 환경의 새로운 정보를 찾아나가는 과정이다.<br>
예를 들어 저녁 메뉴를 선택 할 때 기존에 내가 가봤던 음식점 중 가장 좋았던 곳을 가는 것은 활용, 새로운 음식점을 가는 것은 탐험이라고 볼 수 있다. 만약 내가 항상 가봤던 음식점만 간다면 실패할 가능성은 줄지만  더 나은 음식점을 찾을 수 있는 가능성이 없을 것이다. 이와 같이 활용과 탐험을 적절하게 섞는 것이 중요하다.

### Prediction and Control
마지막으로 강화 학습에서 **예측** (Prediction)은 주어진 정책을 사용했을 때의 현재 상태의 가치를 도출한다. 반면 **컨트롤** (Control)은 가능한 모든 정책들을 예측을 통해 가치를 도출해 내고 이 도출된 가치를 통해 가장 좋은 정책을 찾는 문제이다. 따라서 강의에서는 어떻게 정책을 예측할지를 배운 뒤 정책을 컨트롤 할 수 있는 방법을 배운다.

<h1 id="MDP">Markov Decision Process</h1>
---
2강 Markov Decision Process에서는 강화 학습의 근간이 되는 마코프 결정 프로세스에 대해 배운다. 이를 위해 먼저 마코프 성질, 마코프 프로세스, 마코프 보상 프로세스등을 순차적으로 배우고 마지막에 마코프 결정 프로세스를 배우는데 수식이 조금 많다. 하지만 이번 강의에서 배운 수식 (벨만 방정식 패밀리)들을 그 다음 강의에서 사용하기 때문에 어느 정도의 이해는 해야 강의를 듣는데 수월하지 않을까 생각한다.

## Markov Property
상태 $S_{t}$ 가 마코프 성질을 만족하기 위해선

$$
\mathbb{P}[S_{t+1} \ | \ S_{t}] = \mathbb{P}[S_{t+1} \ | \ S_{1}, ..., S_{t}]
$$

여야 한다. 다시 말하면 마코프 성질을 만족하는 어떤 상태는 과거로부터 필요한 모든 정보들을 담고 있기 때문에 현재 상태를 알면 과거의 정보를 알지 못해도 미래 상태를 예측할 수 있다는 의미이다.

## State Transition Matrix
마코프 성질을 만족하는 상태 $s$ 와, 다음 상태 (Successor state) $s'$ 이 있을 때, 상태 전이 확률은 아래와 같이 정의된다.

$$
P_{ss'} = \mathbb{P}[S_{t+1} \ | \ S_{t}]
$$

또한 상태 전이 행렬 $P$ 는 모든 상태 $s$ 와, 다음 상태 $s'$ 을 이용하여 다음과 같이 표현 할 수 있다.

$$
\qquad \qquad \text{to} \\\\
P = \text{from}
\begin{bmatrix}
        P_{11} & \cdots & P_{1n} \\
        \vdots &  & \vdots \\
        P_{n1} & \cdots & P_{nn} \\
\end{bmatrix}
$$

이 때 각 행의 합은 모두 1이다.

## Markov Process
**마코프 프로세스**는 각각의 상태가 모두 마코프 성질을 갖고 있는 상태의 시퀀스를 의미한다. 마코프 프로세스는 마코프 성질에 의해 memoryless 한 특성을 가지고 있다 (과거를 기억하지 않는다).
또한 마코프 프로세스는 $<S, P>$ 와 같은 튜플의 형태로 구성되며 $S$ 는 모든 상태의 집합을, $P$ 는 상태 전이 확률 행렬을 의미한다.
<div class="imgcap">
<img src="/assets/RL/student_MCTM.png">
<div>[그림2] Student Markov Chain Transition Matrix</div>
</div>
[그림2]와 같이 왼쪽의 예시를 상태 전이 행렬의 형태로 나타내면 오른쪽과 같다. 이 예시에서 나타낼 수 있는 **에피소드**의 예시는 C1, C2, C2, Pass, Sleep 등등이 있다.

## Markov Reward Process
**마코프 보상 프로세스**는 마코프 체인에 가치가 붙은 형태를 의미인데 $<S, P, R, \gamma >$ 인 튜플의 형태로 나타낸다. 이 때 $S, P$ 는 마코프 체인에서와 의미이고 추가된 $R$ 은 $R_{s} = \mathbb{E}[R_{t+1} | S_{t} = s]$ 의 형태를 가지는 보상 함수를, $\gamma$ 는 [0, 1] 사이의 값을 가지는 디스카운트 인자를 의미한다.

또한 이 프로세스의 반환값 (Return) $G_{t}$ 는 시간 $t$ 에서부터 디스카운트된 보상들의 총합과 같다.

$$
G_{t} = R_{t+1} + \gamma R_{t+2} + ... = \sum_{k=0}^{\infty} \gamma^{k}R_{t+k+1}
$$

디스카운트 인자 $\gamma \in [0, 1]$ 는 전체 반환값에서 딜레이된 보상과 즉시 얻은 보상과의 관계를 나타내는데 만약 $\gamma$ 가 0에 가깝다면 현재의 보상만 중요시하는 근시안적인 가치 평가가 될 것이며 1에 가깝다면 (디스카운팅을 하지 않는다면) 먼 미래도 내다 보는 가치 평가가 될 것이다.

강의에 따르면 대부분의 마코프 (보상/결정) 프로세스는 디스카운팅을 한다고 한다. 이 이유로는

- 디스카운트 보상을 사용하면 수학적으로 편리하다.
- 만약 마코프 프로세스에 사이클이 있는 경우 디스카운트 보상을 사용하면 반환값이 무한이 되는 것을 막을 수 있다.
- 사람은 즉각적인 보상을 더 선호하긴 하지만 미래의 보상도 염두해 두면서 행동을 한다.
- 예를 들어 모든 시퀀스가 종료 상태인 경우 디스카운트 하지 않은 마코프 보상 프로세스로는 표현이 불가능 하다.

### Value Function
가치 함수 $v(s)$ 는 상태 $s$ 의 가치를 반환한다. 따라서 가치 함수는 에이전트가 현재 상태에서 얼마나 좋은지를 알려주는 척도라고 볼 수 있다. 그리고 *얼마나 좋은지*는 현재 상태에서 기대할 수 있는 미래의 보상값들로 정의할 수 있기 때문에 반환값 $G_{t}$를 통해 표현할 수 있다.

$$
v(s) = \mathbb{E}[G_{t} \ | \ S_{t}=s]
$$

가치 함수와 반환값에 대한 예시는 [그림3]과 같다.
<div class="imgcap">
<img src="/assets/RL/student_MRP.png" style="max-height:400px">
<div>[그림3] Student MRP</div>
</div>
또한 [그림3]의 예시에서  시작 상태 $S1 = C1$, $\gamma = 1/2$ 인 경우를 가정하고 몇가지 샘플 에피소드의 반환값을 계산하면 아래와 같다.
<div class="imgcap">
<img src="/assets/RL/student_MRP_return.png">
<div>[그림4] Student MRP의 반환값</div>
</div>

### Bellman Equation for MRPs
가치 함수는 아래와 같이 두 개의 파트로 나눌 수 있다.

- 즉각적인 (immediate) 보상 $R_{t+1}$
- 다음 상태들 (Successor state)의 디스카운트 된 가치 $\gamma v(S_{t+1})$

위의 사실들을 이용해서 가치 함수를 쪼갤 수 있는데 그 과정은 아래와 같다.
<div>
\begin{aligned}
v(s) &= \mathbb{E}[G_{t} \ | \ S_{t} = s] \\
     &= \mathbb{E}[R_{t+1} + \gamma R_{t+2} + \gamma^2 R_{t+3} + ... \ | \ S_{t} = s] \\
	 &= \mathbb{E}[R_{t+1} + \gamma(R_{t+2} + \gamma R_{t+3} + ...) \ | \ S_{t} = s] \\
	 &= \mathbb{E}[R_{t+1} + \gamma G_{t+1} \ | \ S_{t} = s] \\
	 &= \mathbb{E}[R_{t+1} + \gamma v(S_{t+1}) \ | \ S_{t} = s]
\end{aligned}
</div>
위의 유도에서 마지막 항을 **벨만 방정식** 이라고 부른다. 이 방정식은 $v(s) = \mathbb{E}[R_{t+1} + \gamma v(S_{t+1}) \ | \ S_{t} = s]$ 의 형태로도 사용할 수도 있고 아래와 같은 식으로도 사용 하기도 한다.

$$
v(s) = R_{s} + \gamma\sum_{s' \in S}P_{ss'}v(s')
$$

위 벨만 방정식을 설명하면 가치 함수는 현재 상태 $s$ 의 즉각적인 보상값과 다음 상태 $s'$ 의 보상값을 합한것과 같다는 의미이다 (물론 $\gamma$ 에 따라 디스카운트 여부를 결정한다). 그리고 상태 $s'$의 가치값은 다시 그 다음 상태 $s''$ 와도 연관이 있는 재귀적인 구조를 가진다.

그리고 이 때 상태 전이 확률 행렬 $P$ 에 의해 자동적으로 각 상태들의 기대값을 계산된다. 따라서 가치 함수를 계산하기 위해서는 상태 전이 확률 행렬을 알아야하는데 이 말은 모든 상태들 간의 전이 확률을 알아야 한다는 의미이다. 일반적인 행렬을 사용한다면 상태 전이 확률을 저장하는데는 $O(n^{2})$ 의 공간이 필요하기 때문에 매우매우 비효율적이다. 이와 관련된 내용은 Function Approximation 강의에서 더 자세히 다루기로 하자.

편의성을 위해 벨만 방정식을 행렬의 형태로 나타내면 다음과 같다.

$$
v = R + \gamma Pv
$$

<div class="imgcap">
<img src="/assets/RL/bellman_matrix_form.png" style="max-height:80px">
</div>
이제 MRP에서 각 상태에서의 가치값은 벨만 방정식의 해를 찾으면 구하는 문제로 바뀌었다. MRP에서의 벨만 방정식은 선형 방정식이기 아래와 같이 역행렬을 구해서 직접적으로 구할 수 있다.

$$
v = (1-\gamma P)^{-1}R
$$

역행렬을 이용하여 선형 방정식의 해를 구하면 상태가 $n$ 개 있을 때 $O(n^3)$ 의 시간 복잡도의 성능을 보인다. 이 때문에 상태의 개수가 매우 작은 간단한 MRP 문제만 직접적으로 해를 구할 수 있다. 그래서 만약 MRP가 복잡하다면 (거의 대부분의 경우) 위와 같이 역행렬을 이용해 직접적으로 해를 구하지 않고 *동적 계획법*을 사용하거나 *몬테-카를로 평가법* 등의 반복법 (Iterative method)을 주로 사용한다.

## Markov Decision Process
**마코프 결정 프로세스**는 마코프 보상 프로세스에 *결정*이 추가로 들어간 형태이며 $<S, A, P, R, \gamma>$ 의 튜플과 정책 $\pi$ 로 구성된다. $A$ 는 행동의 집합을 의미하고 나머지는 마코프 보상 프로세스와 같다.

### Policy
정책 $\pi$ 는 에이전트의 상태가 주어졌을 때 다음번에 수행 할 행동들의 확률, 즉 가능성을 나타낸다.

$$
\pi(a \ | \ s) = \mathbb{P}[A_{t} = a \ | \ S_{t} = s]
$$

우리는 정책을 통해 에이전트의 행동을 정의할 수 있는데 MDP에서의 정책은 마코프 성질에 의해서 과거에 영향 받지 않고 오직 현재 상태에만 의존한다. 만약 어떤 MDP에 $M = <S, A, P, R, \gamma>$ 와 정책 $\pi$ 가 주어졌을 때 상태 시퀀스 $S_{1}, S_{2}, ...$ 는 $<S, P^{\pi}>$ 의 형태를 가지는 마코프 프로세스이고 상태와 보상 시퀀스 $S_{1}, R_{2}, S_{2}, ...$ 는 $<S, P^{\pi}, R^{\pi}, \gamma>$ 의 형태인 마코프 보상 프로세스이다. 이 때,
<div>
\begin{aligned}
P_{s, s'}^{\pi} &= \sum_{a \in A}\pi(a|s)P_{ss'}^{a} \\
R_{s}^{\pi}     &= \sum_{a \in A}\pi(a|s)R_{s}^{a} \\
\end{aligned}
</div>
이며, 이 식을 통해 마코프 결정 프로세스에서의 상태 전이 함수와 보상값은 에이전트가 행하는 정책에 의해 변화하는 것을 볼 수 있다.

### Value Function
**상태-가치 함수** $v_{\pi}(s)$ 는 시작 상태 $s$ 에서 정책 $\pi$ 를 가졌을 때 얻을 수 있는 반환값의 기대값을 의미하며 다음과 같이 표현된다. $v_{\pi}(s) = \mathbb{E_{\pi}}[G_{t} \ \| \ S_{t} = s]$ 사실 정책에 의해 가치 함수가 변화 할 수 있다는 점을 제외하면 MRF 에서의 가치 함수와 동일하다.

반면 정책 $\pi$ 를 따르는 **행동-가치 함수** $q_{\pi}(s)$ 정책 $\pi$ 하에 시작 상태 $s$ 에서 행동 $a$ 을 했을 때 얻을 수 있는 반환값의 기대값을 의미하고 다음과 같이 표현된다. $q_{\pi}(s, a) = \mathbb{E_{\pi}}[G_{t} \ \| \ S_{t} = s, A_{t} = a]$ 다시 말해서 에이전트가 정책 $\pi$ 를 따를 때 상태 $s$ 에서 행동 $a$ 를 했을 때 얼마나 결과가 좋을지를 나타내는 것으로 볼 수 있다.

가치 함수라 칭하면 흔히 상태-가치 함수를 의미하기 때문에 다음부터는 상태-가치 함수를 편의상 가치 함수라 부르겠다. 또한 왜인지는 모르겠지만 흔히 행동-가치 함수를 Q 의 표기법으로 나타내기 때문에 Q-value 함수라고 부르기도 한다.

### Bellman Expectation Equation
MRP에서의 벨만 방정식 유도와 비슷하게 MDP에서도 상태-가치 함수와 행동-가치 상태 $s$ 에서의 즉각적인 보상과 다음 상태 $s'$ 의 가치로 분리될 수 있기 때문에 아래와 같이 벨만 기대값 방정식이 유도 된다.
<div>
\begin{aligned}
v_{\pi}(s)    &= \mathbb{E_{\pi}}[R_{t+1} + \gamma v_{\pi}(S_{t+1}) \ | \ S_{t} = s] \\
q_{\pi}(s, a) &= \mathbb{E_{\pi}}[R_{t+1} + \gamma q_{\pi}(S_{t+1}, A_{t+1}) \ | \ S_{t} = s, A_{t} = a] \\
\end{aligned}
</div>
각각의 가치 함수를 벨만 기대값 방정식으로 표현하면 [그림7-1]과 같다. 먼저 상태-가치 함수는 행동-가치 함수의 값을 에이전트의 정책을 이용하여 weighted sum한 것이며, 행동-가치 함수는 현재 상태와 행동의 즉각적인 보상값과 모든 다음 상태의 상태-가치 함수를 상태 전이 확률 행렬을 이용하여 weighted sum한 것이다.
<div class="imgcap">
<img src="/assets/RL/bellman_MDP_tree1.png" style="max-height:250px">
<img src="/assets/RL/bellman_MDP_tree2.png" style="max-height:250px">
<div>[그림7-1] 벨만 기대값 방정식</div>
</div>
[그림7-2]는 [그림7-1]의 구조에서 상태를 한 단계 더 내다보는 구조인데 내부의 식은 약간 복잡하지만 비슷한 형태이다.
<div class="imgcap">
<img src="/assets/RL/bellman_MDP_tree3.png" style="max-height:250px">
<img src="/assets/RL/bellman_MDP_tree4.png" style="max-height:250px">
<div>[그림7-2] 벨만 기대값 방정식</div>
</div>
예시를 하나 들면 [그림8]를 통해 빨간색의 상태의 가치값이 어떻게 계산되는지를 볼 수 있다 (아마 정책은 모두 0.5, 디스카운트 인자는 1인 것 같다).
<div class="imgcap">
<img src="/assets/RL/bellman_MDP_ex1.png" style="max-height:400px">
<div>[그림8] 벨만 기대값 방정식 예시</div>
</div>
7.4의 가치값은 [그림7-2]의 왼쪽 수식을 이용하여 계산 할 수 있다. 0.5는 정책 $\pi$ 를 의미하고 1은 Pub을 가는 행동의 즉각적인 보상을 의미하며 1을 제외한 괄호 안의 나머지 식은 Pub을 가는 행동을 취했을 때 도달 가능한 모든 상태들 $s'$을 상태 전이 행렬을 이용하여 weight sum을 구해 기대값을 구한 것이다. 0.5*10은 동일하나 종료 상태이기 때문에 다음 상태에 해당되는 식은 표현하지 않는다.

MRP처럼 MDP의 경우도 벨만 기대값 방정식을 행렬의 형태로 나타낼 수 있다. $v_{\pi} = R^{\pi} + \gamma P^{\pi}v_{\pi}$ 의 형태이고, 해의 방정식은 $v_{\pi} = (1 - \gamma P^{\pi})^{-1}R^{\pi}$ 이다.

### Optimal Value Function
**최적의 상태-가치 함수**는 $v_{\*}(s) = \max_{\pi}v_{\pi}(s)$ 와 같이 가장 좋은 정책의 상태-가치 함수를 의미한다. 비슷하게 **최적의 행동-가치 함수**는 $q_{\*}(s, a) = \max_{\pi}q_{\pi}(s, a)$ 로 표현된다. 최적의 가치 함수는 MDP에서 가장 최적의 가치를 가지는 정책이 무엇인지 알려주기 때문에 이 함수의 해를 찾게 된다면 MDP에서 최적의 정책을 구하는 문제를 풀 수 있게 된다.

### Optimal Policy
결국 강화 학습을 푼다는 것은 최적의 정책을 찾는 일로 귀결된다. 그렇기 때문에 어떤 정책이 더 나은가? 와 같은 질문에 답을 할 수 있어야 하는데 이는 정책에 순서 (Ordering)를 매길 수 있다면 가능해지는 일이다. 따라서 정책의 부분 순서 (Partial Order)를 정의하면 정책간의 비교는 아래와 같이 이루어진다.

$$
\pi \ge \pi ' \quad \text{if} \quad v_{\pi}(s) \ge v_{\pi '}(s), \ \forall s
$$

그리고 모든 마코프 결정 프로세스에서

- 만약 최적의 정책 $\pi_{\*}$ 가 있으면 이 정책은 다른 정책들 보다 낫거나 같다는 의미이다.,  $\pi_{\*} \ge \pi , \\ \forall \pi$
- 모든 최적의 정책은 최적의 가치 함수를 도출할 수 있다.,  $v_{\pi_{\*}}(s) = v_{\*}(s)$
- 모든 최적의 정책은 최적의 행동-가치 함수를 도출할 수 있다.,  $q_{\pi_{\*}}(s, a) = q_{\*}(s, a)$

그리고 아래 수식과 같이 

$$
\pi_{*}(a|s) = 
\begin{cases}
1  & \text{if $a = \operatorname{argmax}_{a \in A}q_{*}(s, a)$} \\
0  & \text{otherwise}
\end{cases}
$$

최적의 정책은 최적의 행동-가치 함수를 최대화하는 방향으로 찾을 수 있으며 모든 MDP에서 항상 결정론적인 최적의 정책이 존재한다는 특징이 있다.
<div class="imgcap">
<img src="/assets/RL/optimal_policy_ex1.png" style="max-height:400px">
<div>[그림9] 최적의 정책 예시</div>
</div>
그래서 [그림9] 예시 처럼 모든 가능한 행동에 대해 최적의 행동-가치 함수 $q_{\*}$ 를 미리 구해놓은 상태라면 단순히 $q_{\*}$ 이 높은 행동들을 따라 경로를 그려서 바로 최적의 정책을 구할 수 있다.

### Bellman Optimality Equation
MRP에서는 벨만 방정식을, MDP에서는 벨만 기대값 방정식을 알아보았는데 이제 마지막으로 벨만 최적화 방정식을 알아볼 차례이다. 
<div class="imgcap">
<img src="/assets/RL/bellman_optimality_eq1.png" style="max-height:250px">
<img src="/assets/RL/bellman_optimality_eq2.png" style="max-height:250px">
</div>
<br>
<div class="imgcap">
<img src="/assets/RL/bellman_optimality_eq3.png" style="max-height:300px">
<img src="/assets/RL/bellman_optimality_eq4.png" style="max-height:300px">
<div>[그림10] 벨만 최적화 방정식</div>
</div>
벨만 기대값 방정식과 비슷한 구조로 표현하면 [그림10]과 같다. 최적의 행동-가치 함수는 벨만 기대값 방정식에서의 행동-가치 함수와 같으며 최적의 가치 함수는 최적의 행동-가치 함수의 최대값으로 표현하게 된다 (호의 형태로 나타낸 것은 최대값 연산을 의미한다). 

<div class="imgcap">
<img src="/assets/RL/bellman_optimality_eq_ex1.png" style="max-height:400px">
<div>[그림11] 벨만 최적화 방정식 예시</div>
</div>
벨만 최적화 방정식을 이용하여 상태의 최적 가치 함수를 도출하면 [그림11]과 같다. 현재 상태의 최적의 가치 함수는 행동을 할 때 오는 즉각적인 보상과 그 다음 상태의 최적의 가치 함수를 통해서 재귀적으로 구할 수 있으며, 또 구해진 최적의 가치 함수를 이용해서 최적의 상태-가치 함수를 구할 수 있다. [그림10]의 예시에서는 모든 $q_{\*}$ 가 구해진 상태를 가정했지만 이제 벨만 최적화 방정식을 이용하면 $q_{\*}$ 도 직접 구할 수 있는 것이다.

MRP와 마찬가지로 MDP에서도 벨만 방정식을 이용해 가치 함수를 찾을 수 있는 식을 만들었지만 문제는 벨만 최적화 방정식은 비선형 방정식이라는 것에서 차이를 보인다. 일반적으로 비선형방정식은 쉽게 풀 수 없기 때문에 분석적인 방법을 통해 구하기는 어렵고 대신 반복적 방법 예를들어 *가치 반복 (Value Iteration)*, *정책 반복 (Policy Iteration)*, *Q-학습*, *Sarsa*등을 사용한다.

<h1 id="Planning">Planning by Dynamic Program</h1>
---
1, 2강에서 강화 학습의 근간이 되는 용어들과 MDP에 대해서 공부하였다. 이제 3강부터 어떻게 MDP의 해를 찾을 수 있을지에 대해 공부한다. 3강에서는 에이전트가 모델을 알고 있다는 가정 하에 MDP의 해를 구하는 planning에 대해 알아 본다. 그리고 4, 5강은 모델에 대한 지식이 필요 없는 비-모델 강화 학습을 배우게 된다.

## Dynamic Programming
동적 계획법 (Dynamic Programming)은 복잡한 문제를 풀기 위한 방법이다. 분할-정복법과 비슷하게 복잡한 문제를 여러개의 작은 문제로 쪼갠 뒤 작은 문제를 풀고 합치는 방법을 통해 문제를 해결한다. 아래와 같은 특징을 가지고 있는 문제에서는 동적 계획이 많이 쓰인다.

- 최적의 부분 구조: 최적화의 원칙에 의해 최적의 해는 여러개의 부분 문제로 분할할 수 있다.
- 부분 문제의 반복: 부분 문제를 여러번 구해야 하는 상황이 생기므로 부분 문제의 해를 캐싱해놓고 재사용할 수 있다.

여기서 동적 계획법과 분할-정복법의 차이는 부분 문제의 반복 특징을 적용하느냐에 달려있는데, 반복해서 등장하는 부분 문제를 미리 계산하고 캐싱하기 때문에 원래 문제의 시간 복잡도를 매우 단축할 수 있지만 캐싱 문제에 의해 메모리를 많이 차지하는 단점도 있다.

동적 계획법은 MDP의 모든 정보를 알 경우에 적용할 수 있으며 이는 MDP의 계획법 (Planning)에 사용된다. 이 중 *예측 (Prediction)* 문제는 입력으로 MRP 혹은 정책 $\pi$ 와 MDP를 받으며 이를 이용하여 가치 함수 $v_{\pi}$ 를 도출해낸다. 반면 *컨트롤* 문제에서는 정책을 제외한 MDP를 입력으로 받고 출력은 최적의 가치 함수 $v_{\*}$ 와 최적의 정책 $\pi_{*}$ 를 도출한다.

MDP의 해를 구할 때 강화 학습을 사용하는 이유는 MDP의 해를 구하는 작업이 재귀적으로 일어나기 때문이다. 예를 들어 현재 상태의 가치값은 다음 상태의 가치값을 알아야 하며 다음 상태의 가치값은 다음 다음 상태의 가치값을 알아야 구할 수 있는 것 처럼 말이다. 따라서 일반적인 방법을 사용한다면 모든 값들을 다시 계산하기 때문에 성능이 매우 떨어진다. 그렇기 때문에 어느정도의 메모리를 포기하더라도 동적 계획법을 사용하는 것이다.

## Policy Evaluation
**정책 평가** (Policy Evaluation)는 정책 $\pi$ 가 주어졌을 때 이 정책이 얼만큼 가치가 있는지를 평가하는 것이다. 이를 구하기 위해서 벨만 기대값 방정식을 동적 계획법을 이용하여 반복적으로 계산하는 방법을 사용한다. 정책 평가를 동기화적 백업 방법 (Synchronous backups)을 사용한다면,

1. 매 이터레이션 $k$ 마다 모든 상태 $s \in S$ 을 순회하며,
2. $v_{k}(s')$ 를 이용해서 $v_{k+1}(s)$ 업데이트<br>
이 때 $s'$ 는 상태 $s$ 의 다음 상태 (Successor state)를 의미

와 같다. 증명을 하진 않았지만 반복적 정책 평가는 잘 수렴 한다고 한다. 그리고 반복적 정책 평가를 수식으로 더 자세히 표현한다면,
<div>
\begin{aligned}
v_{k+1}(s) &= \sum_{a \in A}\pi(a \ | \ s)(R_{s}^{a} + \gamma \sum_{s' \in S}P_{ss'}^{a}v_{k}(s')) \\
\boldsymbol{v}^{k+1} &= \boldsymbol{R^{\pi}} + \gamma \boldsymbol{P^{\pi}}\boldsymbol{v}^{k}
\end{aligned}
</div>
인데, 정책 $\pi$ 일 때 $k+1$ 번째 이터레이션의 가치값은 이전 이터레이션의 가치값을 벨만 기대값 방정식에 집어 놓은 형태이다. 따라서 <br>
정책 평가를 예제를 통해 알아보기 위해 아래와 같은 간단한 그리드 세계를 생각해보자.
<div class="imgcap">
<img src="/assets/RL/policy_eval_grid_world.png" style="max-height:200px">
<div>[그림12] 그리드 세계</div>
</div>

- $\gamma$ 값이 1인 MDP이며, 회색 상태는 종료 상태를 의미한다.
- 보상은 모두 -1 이다.
- 에이전트는 랜덤 정책을 따른다 (동서남북 각각 방향으로 이동할 확률은 모두 0.25 이다).

<div class="imgcap">
<img src="/assets/RL/policy_eval_result1.png" style="max-height:350px">
&nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp;
<img src="/assets/RL/policy_eval_result2.png" style="max-height:350px">
<div>[그림13] 정책 평가 예시 </div>
</div>
k=0일 때는 모든 상태의 가치값을 0.0으로 초기화 하고, 첫 이터레이션 (k=1)에서는 $\boldsymbol{v}^{k+1} = \boldsymbol{R^{\pi}} + \gamma \boldsymbol{P^{\pi}}\boldsymbol{v}^{k}$ 공식을 이용해서 모든 상태들의 가치값을 업데이트 한다. 예를들어 [0,1] 번재 상태의 업데이트는 -1 + (0.0+0.0+0.0)/3 이기 때문에 -1.0이 나오게 되고 비슷하게 두번재 이터레이션의 [0, 1]번째 상태는 -1 + (0.0-1.0-1.0)/3 이므로 -1.75의 가치값을 가지게 된다. 이 방식으로 계속해서 가치값을 업데이트하게 된다면 어느 시점부터 수렴하게 된다.

## Policy Iteration
위의 예시는 정책을 고정시키고 가치값을 반복적으로 업데이트하면서 수렴해 가는 정책 평가를 통해 정책 $\pi$ 일 때 모든 상태의 가치값을 구했다. 하지만 만약 정책을 최상의 정책으로 만들기 위해서는 정책 평가를 통해 얻어낸 가치를 이용하여 정책을 업데이트 해야 한다. 이러한 방식을 **정책 반복** (Policy Iteration)이라 부른다.
<div class="imgcap">
<img src="/assets/RL/policy_eval_result3.png" style="max-height:350px">
<img src="/assets/RL/policy_eval_result4.png" style="max-height:350px">
<div>[그림14] 정책 반복 예시</div>
</div>
정책 반복은 정책 평가를 통해 도출한 상태의 가치값을 이용해 **그리디**한 방식으로 정책을 업데이트 한다. 위 예시에서는 정책을 랜덤하게 초기화를 했기 때문에 k=0 인 경우 랜덤 정책을 가지고 있다. k=1 인 이터레이션에서는 평가된 정책의 가치값을 이용해 현재 갈 수 있는 상태 중 가장 높은 가치값을 가지고 있는 상태로 행동을 할 수 있도록 정책을 수정한다.

이 예제는 매우 작은 예제이기 때문에 k=3 이후부터 정책이 최적화된 정책으로 수렴하는 것을 볼 수 있다. 하지만 실제 세계 혹은 복잡한 세계에서는 더 많은 이터레이션이 필요로 하다. 또한 정책 평가와 마찬가지로 정책 반복도 최적화된 정책으로 수렴한다는 것으로 알려져 있다.

사실 위 예제가 정확하게 정책 반복을 표현하고 있는 거 같진 않은점이, 정책 반복은 *정책 평가의 수렴된 값 $v_{\pi}$ 를 이용하여 그리디한 방식을 통해 정책을 업데이트 하는 것 이다.* 하지만 위 예제에서는 마치 매 이터레이션 마다 정책 평가의 업데이트와 정책 수정이 동시에 일어나는 것 처럼 표현을 한다.

다시 정리하자면 정책 반복은 아래 의사코드와 같다.

```python
# 정책 반복
while not policy is optimal: # 정책이 최적의 정책으로 수렴할 때 까지
    k = 0
    # 정책 평가
    while not v is converge: # 가치값이 수렴할 때 까지
        # Note: 모든 상태에 대해 수행함!
        v = R + gamma * P * v        
    policy = argmax(v)   # 그리드 정책 업데이트
```

## Policy Improvement

## Value Iteration



# Reference
[UCL Course on RL by David Silver](http://www0.cs.ucl.ac.uk/staff/d.silver/web/Teaching.html)




