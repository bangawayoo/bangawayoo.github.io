---
layout: "post"
title: "notes1"
excerpt: "notes1"
date: "2016-05-21 05:00:00"
---

# Word Vectors
---
NLP 문제에서 가장 중요한 작업은 입력 받은 단어를 어떻게 표현할지 정하는 것이다. 이에 대해 많은 연구가 진행되었지만 최근 연구들은 대부분 단어를 벡터의 형태로 저장하는 추세이다. 단어들을 **단어 벡터**의 형태로 저장한다면 단어간의 연산도 할 수 있고, 단어 사이의 연관성도 쉽게 볼 수 있는 장점이 있기 떄문이다. 참고로 단어를 단어 벡터로 변환하는 과정을 **단어 임베딩**이라고 한다.

## One-hot Encoding
단어를 벡터로 표현하는 가장 쉽고 단순한 방법은 **one-hot 인코딩** 방법일 것이다. one-hot 인코딩은 단어를 아래와 같이 표현하는 방법이다.
<div>
\begin{align}
w^{\text{aardvark}} = 
\begin{bmatrix}
1 \\ 0 \\ 0 \\ \vdots \\ 0 \\
\end{bmatrix}
,
w^{\text{a}} = 
\begin{bmatrix}
0 \\ 1 \\ 0 \\ \vdots \\ 0 \\
\end{bmatrix}
,\cdots,
w^{\text{zebra}} = 
\begin{bmatrix}
0 \\ 0 \\ 0 \\ \vdots \\ 1 \\
\end{bmatrix}
\end{align}
</div>
단어들은 $\mathbb{R}^{V \times 1}$ 인 벡터로 표현이 되고 (이 때 $V$ 는 전체 단어의 수), 0 또는 1로 표현할 수 있다. 이 방법을 사용하면 간단하게 단어들을 벡터의 형태로 나타낼 수 있지만, 단어들간의 연관성을 표현하지 못한다는 문제점을 가지고 있다.

## Co-occurrence Matrix
우리는 단어의 주변 컨텍스트에 의해 단어의 의미를 추론할 수 있다. 예를 들어 "I eat apple.", "Apple launched new iPhone." 와 같은 두 문장이 있을 때 우리는 전자의 apple은 과일, 후자의 애플은 회사임을 주변 컨텍스트에 의해 유추한다. 이러한 가정을 바탕으로 문서 혹은 말뭉치(corpus) 에서 모든 단어들에 대해 Co-occurrence 행렬을 만들고 이 행렬을 이용해 단어 임베딩을 한다면 효과적일 것이다.

이 행렬은 Word-Document 행렬과 Window-based matrix 두 가지 방식이 있다.<br>
당연하게도 문서에서 자주 나오는 단어들은 서로 연관되어 있다. 예를 들어 A 라는 문서는 주로 "은행", "채권", "주식", "현금" 등의 단어가 자주 나오며 사람들은 이 문서를 보고 "경제" 와 관련된 문서라고 쉽게 유츄한다. 반면 뜬금없이 "은행" 과 "문어" 가 같이 나오는 일은 상대적으로 적을 것 이다. 이 같은 사실을 바탕으로 아래와 같이 Word-Document 행렬을 만들 수 있다.

```
for i in documents:
    for j in words:
	    X[i, j] += 1, if 단어 j가 문서 i에 있을 때
```

이렇게 만들어지는 행렬은 $\mathbb{R}^{V \times M}$ 인 매우 큰 차원을 가질뿐더러, 문서의 개수가 많아지면 많아질 수록 행렬의 크기는 더 커진다. 물론 이 행렬을 사용한다면 각 문서의 일반적인 주제를 추론할 수 있는 Latent Semantic Analysis 와 같은 문제를 처리하는데 용이하지만 이 강의에서는 주로 다루진 않는다.

반면 Window-based Co-occurrence 행렬은 단어가 나타나는 횟수를 문서 전체가 아닌 특정한 크기를 갖는 window 내에서만 기록한다. 예를 들어

1. I enjoy flying.
2. I like NLP.
3. I like deep learning.

와 같은 문장이 있고 window 의 크기는 1이라고 가정한다면 만들어지는 행렬은 아래와 같다.
<div class="imgcap">
<img src="/assets/note1/matrix1.png" style="max-width:480px">
</div>

Co-occurrence 행렬을 이용해 단어 임베딩을 했지만 아직 큰 문제가 있다. 위 예시에서 행렬의 크기는 $V \times V$ 이다. 이 때문에

1. 사전의 단어 수가 늘어나면 행렬이 커진다.
2. 행렬의 차원이 매우 크기 때문에 저장 공간이 많이 필요하다.
3. 행렬 자체가 매우 sparse 하기 때문에 임베딩된 단어를 다른 문제에 적용시킬 경우 robust 하지 못하다.

Sparsity 에 대한 이슈를 어떻게 해결해야 할까? 가장 쉬운 방법은 행렬의 차원을 줄이는 것이다.<br>
모든 정보를 담고 있는 **sparse** 한 벡터 대신 약간의 정보는 잃더라도 중요한 정보를 담고 있는 **dense** 한 벡터를 만들어 사용하자는 의미이다. 이러한 방법을 **차원 축소**라고 부르기도 하며 통상적으로 단어 임베딩에서는 25~1000 차원 정도의 벡터로 축소한다. 그럼 차원 축소는 어떤 방법을 사용해야 할까? 방법이야 많이 있겠지만 이 강의에서는 그 중에서 자주 사용되는 SVD, word2vec, Glove 의 방법을 소개한다.

## SVD based Method
SVD 는 Singular Value Decomposition 을 의미하며, 대충 말해서 $n \times m$ 행렬 $X$ 를 아래와 같이 분해하는 방법이다.

$$
X = U \Sigma V^T
$$

이 때, $U$ 는 $AA^T$ 행렬을 Eigen Value Decomposition 한 결과, $V^T$ 는 $A^TA$ 행렬을 EVD 한 결과이고 $\Sigma$ 는 EVD에 의해 도출된 Singular Value 를 의미한다. 더 자세한 내용은 [블로그](http://darkpgmr.tistory.com/106) 에서 쉽게 설명해 놓았으니 참고하면 될 것 같다.

위에서 얻은 Co-occurrence 행렬을 SVD 를 통해 $U \Sigma V^T$ 꼴로 분해하고, 얻은 행렬 $U$ 를 단어 임베딩 벡터로 사용하면 끝이다. 대신 $U$ 행렬을 그대로 사용하지 않고 축소할 차원만큼 잘라서 사용하게 된다.
<div class="imgcap">
<img src="/assets/note1/SVD.png" style="max-width:540px">
</div>
행렬 $X$ 는 원본 Co-occurrence 행렬이고 아래 $\hat{X}$ 행렬은 SVD 를 이용해 차원 축소를 한 행렬이다. 잘라낸다는 의미는 행렬 $U$ 의 차원을 $n \times r$ 에서 $n \times k$ 로 축소하는 것 이다 (이 때, $k \\ << \\ r$). 잘라낼 개수 $k$ 는 아래 공식과 같이 Singular Value 의 비율을 이용해서 적당한 값을 얻어낼 수 있다.

$$
\frac{\sum^k _{i=1} \sigma _i}{\sum^V _{i=1} \sigma _i}
$$

SVD 를 잘 몰라서 그런 것 같기도 한데 여기서 드는 의문은 왜 $U$ 를 임베딩에 사용하는 것인지 궁금하다. $V$ 를 사용하면 안되나?<br>
어쨌거나 이렇게 차원 축소된 행렬도 의미론적, 문법론적에 대해 충분한 정보가 인코딩되어 있게 되지만 SVD 를 이용한 차원 축소는 아래와 같은 단점이 있다.

- SVD 를 계산하는데 $O(nm^2)$ 만큼 든다. 
- 새로운 단어나 문서의 추가에 취약하다. (잘 이해가 되지 않는데 아마 새로운 단어나 문서가 추가되면 SVD 를 다시 돌려야 한다는 의미인가?)
- 대부분의 단어들은 서로 co-occur 하지 않기 때문에 행렬이 매우 sparse 하다.
- 매우 자주 등장하는 단어 (the, he, has) 들에 의해 문제점이 발생할 수 있다.

마지막 문제점은 아래와 같은 약간의 트릭을 써서 어느정도 완화할 수 있다.

- Stop word 등을 제거하고 임베딩 한다.
- Ramp window 를 적용해 본다. 이 말은 Co-occurrence 를 카운팅할 때 거리에 따라 weight 를 달리 하는 방법이다.
- 단순 카운트 대신 피어슨 상관계수를 사용하고 음수 값은 0으로 치환하여 행렬을 만든다.

등등 여러가지 방법이 있다. 하지만 나머지 문제점은 해결하지 못하기 때문에 *backpropagation* 이라는 방식을 이용한 Iteration-based 방법을 주로 사용하게 된다.

## Iteration based Method
- CBOW
- **Skip-gram** <- negative sampling


